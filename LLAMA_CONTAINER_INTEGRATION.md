# LLaMA Server Container Integration

## âœ… Files Now Included in Docker Image

### Core LLaMA Components
- `setup_llama_server.py` - Main LLaMA server management
- `scripts/llama_runner.py` - Model download and server control
- `start_llama.sh` - Quick start script
- `quick_start_llama.py` - Python quick setup
- `test_llama_integration.py` - Integration testing
- `container_startup.py` - Container-specific startup logic

### Dependencies Added to requirements-minimal.txt
```
llama-cpp-python>=0.2.55
sentence-transformers>=2.5.0  
chromadb>=0.4.22
```

### Container Startup Flow
1. **Phase 1**: Container starts with `container_startup.py`
2. **Phase 2**: Automatically checks for/downloads LLaMA models
3. **Phase 3**: Starts LLaMA server in background (if models available)
4. **Phase 4**: Launches main Streamlit application with LLaMA integration

### Environment Variables
```bash
K8S_AI_MODE=interactive
K8S_AI_LLAMA_ENABLED=true
SENTENCE_TRANSFORMERS_HOME=/opt/models
HF_HOME=/opt/models
```

## ğŸš€ How to Use

### Quick Start
```bash
# Build image with LLaMA support
docker build -t k8s-ai-agent:llama .

# Run with LLaMA server
docker run -p 8080:8080 k8s-ai-agent:llama
```

### Access Points
- **Main Dashboard**: http://localhost:8080
- **LLaMA Server API**: http://localhost:8080/v1/models (when running)
- **Health Check**: http://localhost:8080/health

### Features Enabled
âœ… Interactive AI chat with LLaMA models
âœ… Automatic model download on first run  
âœ… Kubernetes troubleshooting with AI
âœ… GlusterFS analysis with AI
âœ… Real-time system monitoring
âœ… Expert remediation suggestions

## ğŸ”§ Troubleshooting

### If LLaMA Server Shows Offline
The container startup will:
1. Auto-download models if none exist (~4GB, takes time)
2. Start server automatically in background
3. Dashboard will show "Online" once ready

### Check Container Logs
```bash
docker logs <container-name>
```

### Manual LLaMA Setup Inside Container
```bash
docker exec -it <container-name> bash
python setup_llama_server.py --setup
```

## ğŸ“Š What You Get

### Before (Missing LLaMA Files)
âŒ LLaMA server files not copied to container
âŒ llama-cpp-python not installed
âŒ Models not available
âŒ Dashboard shows "LLaMA Server Offline"

### After (Complete Integration)  
âœ… All LLaMA components included in image
âœ… Dependencies installed during build
âœ… Automatic model download and server start
âœ… Interactive AI chat interface ready
âœ… Kubernetes + GlusterFS AI troubleshooting enabled

Your interactive AI assistant is now fully packaged and ready to deploy! ğŸš€
