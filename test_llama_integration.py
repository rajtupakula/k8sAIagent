#!/usr/bin/env python3
"""
Test LLaMA Server Integration
Verifies that the LLaMA server can be used for live K8s troubleshooting
"""
import sys
import os
import time
import requests

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_llama_server():
    """Test LLaMA server functionality"""
    print("ğŸ”¬ Testing LLaMA Server Integration")
    print("=" * 40)
    
    # Test 1: Server health check
    print("1. ğŸ¥ Checking server health...")
    try:
        response = requests.get("http://localhost:8080/health", timeout=5)
        if response.status_code == 200:
            print("   âœ… Server is healthy")
        else:
            print(f"   âŒ Server returned status {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ Server not responding: {e}")
        print("   ğŸ’¡ Start server with: python3 setup_llama_server.py --start")
        return False
    
    # Test 2: Basic query
    print("2. ğŸ§ª Testing basic query...")
    try:
        test_payload = {
            "prompt": "Hello, are you working correctly?",
            "max_tokens": 30,
            "temperature": 0.1
        }
        
        response = requests.post(
            "http://localhost:8080/v1/completions",
            json=test_payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("choices", [{}])[0].get("text", "")
            print(f"   âœ… Response: {response_text.strip()}")
        else:
            print(f"   âŒ Query failed with status {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ Query failed: {e}")
        return False
    
    # Test 3: Kubernetes-specific query
    print("3. ğŸš¢ Testing Kubernetes query...")
    try:
        k8s_payload = {
            "prompt": "What kubectl command shows all pods in all namespaces?",
            "max_tokens": 50,
            "temperature": 0.1
        }
        
        response = requests.post(
            "http://localhost:8080/v1/completions",
            json=k8s_payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("choices", [{}])[0].get("text", "")
            print(f"   âœ… K8s Response: {response_text.strip()}")
        else:
            print(f"   âŒ K8s query failed with status {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ K8s query failed: {e}")
        return False
    
    # Test 4: RAG Agent integration
    print("4. ğŸ¤– Testing RAG Agent integration...")
    try:
        from agent.rag_agent import RAGAgent
        
        # Create RAG agent without forcing offline mode
        rag_agent = RAGAgent(offline_mode=False)
        
        if hasattr(rag_agent, 'llama_available') and rag_agent.llama_available:
            print("   âœ… RAG Agent can connect to LLaMA server")
            
            # Test expert query
            result = rag_agent.expert_query("Test LLaMA integration", auto_remediate=False)
            if result and result.get('confidence', 0) > 0:
                print(f"   âœ… Expert query successful (confidence: {result.get('confidence', 0):.1%})")
            else:
                print("   âš ï¸  Expert query returned low confidence")
        else:
            print("   âŒ RAG Agent cannot connect to LLaMA server")
            print(f"   ğŸ“Š Offline mode: {getattr(rag_agent, 'offline_mode', 'Unknown')}")
            return False
            
    except Exception as e:
        print(f"   âŒ RAG Agent test failed: {e}")
        return False
    
    print("\nğŸ‰ All tests passed! LLaMA server is ready for live commands and remediation.")
    return True

def test_dashboard_integration():
    """Test dashboard integration"""
    print("\nğŸ–¥ï¸  Testing Dashboard Integration")
    print("=" * 40)
    
    try:
        from ui.advanced_dashboard import AdvancedDashboardUI
        
        # Create dashboard instance
        dashboard = AdvancedDashboardUI()
        
        # Check agent status
        if dashboard.rag_agent and hasattr(dashboard.rag_agent, 'llama_available'):
            if dashboard.rag_agent.llama_available:
                print("âœ… Dashboard RAG agent connected to LLaMA server")
            else:
                print("âš ï¸  Dashboard RAG agent in offline mode")
        else:
            print("âŒ Dashboard RAG agent not properly initialized")
            return False
        
        # Test server status check
        server_status = dashboard._check_llama_server_status()
        if server_status.get("running"):
            print("âœ… Dashboard can detect running LLaMA server")
        else:
            print("âŒ Dashboard cannot detect LLaMA server")
            return False
        
        print("âœ… Dashboard integration working correctly")
        return True
        
    except Exception as e:
        print(f"âŒ Dashboard integration test failed: {e}")
        return False

def main():
    """Main test function"""
    print("ğŸš€ K8s AI Agent - LLaMA Server Integration Test")
    print("=" * 50)
    
    # Test server
    server_ok = test_llama_server()
    
    if server_ok:
        # Test dashboard integration
        dashboard_ok = test_dashboard_integration()
        
        if dashboard_ok:
            print("\nğŸ‰ SUCCESS: LLaMA server integration is working perfectly!")
            print("\nğŸ“‹ Next Steps:")
            print("1. Start Advanced Dashboard: streamlit run ui/advanced_dashboard.py")
            print("2. Look for 'AI Agent Active (Online Mode)' status")
            print("3. Try asking complex Kubernetes questions in the chat")
            print("4. Use the expert action buttons for live remediation")
            print("\nğŸ’¡ The system is now capable of:")
            print("   â€¢ Live command generation and execution")
            print("   â€¢ Context-aware troubleshooting")
            print("   â€¢ Automated remediation workflows")
            print("   â€¢ Real-time AI-powered analysis")
        else:
            print("\nâš ï¸  Server works but dashboard integration has issues")
    else:
        print("\nâŒ LLaMA server is not available")
        print("\nğŸ”§ To fix this:")
        print("1. Run: python3 setup_llama_server.py --setup")
        print("2. Or: ./start_llama.sh")
        print("3. Wait for model download and server startup")
        print("4. Run this test again")

if __name__ == "__main__":
    main()
