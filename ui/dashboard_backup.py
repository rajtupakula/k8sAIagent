#!/usr/bin/env python3
"""
INTERACTIVE KUBERNETES AI AGENT DASHBOARD
Expert-level troubleshooting assistant with real-time chat capabilities
"""
import streamlit as st
import os
import sys
import time
import json
import subprocess
import logging
import re
from datetime import datetime
from typing import Dict, List, Any
import threading

# Configure Streamlit page FIRST
st.set_page_config(
    page_title="ü§ñ Kubernetes AI Agent - Interactive Assistant",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Try importing Kubernetes components with graceful fallback
try:
    sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'agent'))
    from monitor import KubernetesMonitor
    from remediate import RemediationEngine
    K8S_COMPONENTS_AVAILABLE = True
except ImportError as e:
    K8S_COMPONENTS_AVAILABLE = False
    KubernetesMonitor = None
    RemediationEngine = None

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# EXPERT AI KNOWLEDGE BASE - Your personal Kubernetes troubleshooting expert
EXPERT_KNOWLEDGE = {
    "pod_issues": {
        "crashloopbackoff": {
            "analysis": "üîç **Root Cause Analysis**: Pod is stuck in restart loop - application failing to start properly",
            "confidence": 0.95,
            "urgency": "HIGH",
            "solutions": [
                "üîé **Immediate**: Check logs: `kubectl logs <pod-name> --previous`",
                "üè∑Ô∏è **Verify**: Container image and tag are correct and exist",
                "‚öôÔ∏è **Config**: Validate environment variables and config maps",
                "üìä **Resources**: Check if CPU/memory limits are sufficient",
                "üåê **Network**: Ensure health check endpoints are responding",
                "üîê **Security**: Verify service account permissions",
                "üíæ **Storage**: Check if required volumes are mounted correctly"
            ],
            "prevention": "Implement proper health checks and resource monitoring",
            "related_issues": ["imagepullbackoff", "oomkilled"]
        },
        "pending": {
            "analysis": "‚è≥ **Root Cause Analysis**: Pod cannot be scheduled - resource or constraint issues",
            "confidence": 0.90,
            "urgency": "MEDIUM",
            "solutions": [
                "üñ•Ô∏è **Nodes**: Check node capacity: `kubectl describe nodes`",
                "üìà **Resources**: Verify resource requests vs available capacity",
                "üéØ **Affinity**: Review node selectors and affinity rules", 
                "üíæ **Storage**: Ensure PVCs and storage classes are available",
                "üè∑Ô∏è **Taints**: Check for node taints blocking scheduling",
                "üîí **Security**: Validate pod security policies"
            ],
            "prevention": "Monitor cluster capacity and set appropriate resource requests",
            "related_issues": ["insufficient_resources", "node_not_ready"]
        },
        "imagepullbackoff": {
            "analysis": "üñºÔ∏è **Root Cause Analysis**: Cannot pull container image - registry or auth issues",
            "confidence": 0.92,
            "urgency": "HIGH",
            "solutions": [
                "üîç **Registry**: Verify image name, tag, and registry accessibility",
                "üîë **Auth**: Check image pull secrets and registry credentials",
                "üåê **Network**: Test connectivity to image registry",
                "‚è±Ô∏è **Rate Limits**: Check for registry rate limiting",
                "üè∑Ô∏è **Tags**: Ensure image tag exists (avoid 'latest' in production)"
            ],
            "prevention": "Use private registry mirrors and implement proper image management",
            "related_issues": ["authentication_failure"]
        },
        "oomkilled": {
            "analysis": "üí• **Root Cause Analysis**: Container killed due to memory exhaustion",
            "confidence": 0.88,
            "urgency": "HIGH", 
            "solutions": [
                "üìä **Memory**: Increase memory limits in deployment spec",
                "üîç **Profiling**: Analyze application memory usage patterns",
                "‚öôÔ∏è **Optimization**: Optimize application memory consumption",
                "üìà **Monitoring**: Implement memory usage alerts",
                "üéØ **Requests**: Set appropriate memory requests for scheduling"
            ],
            "prevention": "Proper load testing and memory profiling before deployment",
            "related_issues": ["resource_exhaustion"]
        }
    },
    "node_issues": {
        "notready": {
            "analysis": "üñ•Ô∏è **Root Cause Analysis**: Node unavailable for pod scheduling - kubelet or system issues",
            "confidence": 0.85,
            "urgency": "CRITICAL",
            "solutions": [
                "üîß **Kubelet**: Check kubelet logs: `journalctl -u kubelet`",
                "üåê **Network**: Verify node network connectivity and CNI",
                "üíæ **Disk**: Check disk space and inode usage",
                "üîÑ **Restart**: Consider kubelet service restart",
                "üê≥ **Runtime**: Verify container runtime status"
            ],
            "prevention": "Implement node monitoring and automated health checks",
            "related_issues": ["network_issues", "disk_pressure"]
        },
        "disk_pressure": {
            "analysis": "üíæ **Root Cause Analysis**: Node running out of disk space",
            "confidence": 0.90,
            "urgency": "HIGH",
            "solutions": [
                "üßπ **Cleanup**: Remove unused images: `docker system prune`",
                "üìä **Analysis**: Identify large files and logs",
                "üìà **Monitoring**: Set up disk usage alerts",
                "üîÑ **Rotation**: Implement log rotation policies"
            ],
            "prevention": "Proactive disk monitoring and automated cleanup",
            "related_issues": ["log_overflow"]
        }
    },
    "service_issues": {
        "no_endpoints": {
            "analysis": "üåê **Root Cause Analysis**: Service has no healthy backend pods",
            "confidence": 0.88,
            "urgency": "HIGH",
            "solutions": [
                "üéØ **Selectors**: Verify service selector matches pod labels",
                "‚ù§Ô∏è **Health**: Check pod readiness probes",
                "üîç **Pods**: Ensure pods are running and ready",
                "üåê **Ports**: Verify service and container port configuration"
            ],
            "prevention": "Implement proper health checks and service monitoring",
            "related_issues": ["pod_not_ready"]
        }
    }
}

# Initialize session state for chat and monitoring
if 'ai_chat_history' not in st.session_state:
    st.session_state.ai_chat_history = []
if 'monitor' not in st.session_state:
    st.session_state.monitor = None
if 'remediation' not in st.session_state:
    st.session_state.remediation = None
if 'last_health_check' not in st.session_state:
    st.session_state.last_health_check = {}
if 'expert_mode' not in st.session_state:
    st.session_state.expert_mode = True

def initialize_components():
    """Initialize Kubernetes components"""
    if not K8S_COMPONENTS_AVAILABLE:
        return False
    
    try:
        if st.session_state.monitor is None:
            st.session_state.monitor = KubernetesMonitor()
        
        if st.session_state.remediation is None:
            st.session_state.remediation = RemediationEngine()
        
        return True
    except Exception as e:
        st.error(f"Failed to initialize Kubernetes components: {e}")
        return False

def expert_ai_analysis(user_query: str) -> Dict[str, Any]:
    """Advanced AI analysis with expert-level insights"""
    query_lower = user_query.lower()
    
    # Advanced pattern matching with context awareness
    analysis_result = {
        "query": user_query,
        "expert_analysis": "",
        "solutions": [],
        "confidence": 0.0,
        "urgency": "LOW",
        "issue_type": "unknown",
        "prevention": "",
        "related_issues": [],
        "commands": [],
        "next_steps": []
    }
    
    # CrashLoopBackOff Analysis
    if any(word in query_lower for word in ["crashloop", "crash", "restarting", "restart loop", "exit code"]):
        issue_data = EXPERT_KNOWLEDGE["pod_issues"]["crashloopbackoff"]
        analysis_result.update({
            "issue_type": "crashloopbackoff",
            "expert_analysis": issue_data["analysis"],
            "solutions": issue_data["solutions"],
            "confidence": issue_data["confidence"],
            "urgency": issue_data["urgency"],
            "prevention": issue_data["prevention"],
            "related_issues": issue_data["related_issues"],
            "commands": [
                "kubectl get pods -A | grep -E '(CrashLoop|Error)'",
                "kubectl logs <pod-name> --previous",
                "kubectl describe pod <pod-name>"
            ]
        })
    
    # Pending Pod Analysis  
    elif any(word in query_lower for word in ["pending", "scheduling", "unschedulable", "cannot schedule"]):
        issue_data = EXPERT_KNOWLEDGE["pod_issues"]["pending"]
        analysis_result.update({
            "issue_type": "pending",
            "expert_analysis": issue_data["analysis"],
            "solutions": issue_data["solutions"],
            "confidence": issue_data["confidence"],
            "urgency": issue_data["urgency"],
            "prevention": issue_data["prevention"],
            "commands": [
                "kubectl get pods -A | grep Pending",
                "kubectl describe pod <pod-name>",
                "kubectl get nodes -o wide",
                "kubectl top nodes"
            ]
        })
    
    # ImagePullBackOff Analysis
    elif any(word in query_lower for word in ["imagepull", "image", "pull", "registry", "unauthorized"]):
        issue_data = EXPERT_KNOWLEDGE["pod_issues"]["imagepullbackoff"]
        analysis_result.update({
            "issue_type": "imagepullbackoff",
            "expert_analysis": issue_data["analysis"],
            "solutions": issue_data["solutions"],
            "confidence": issue_data["confidence"],
            "urgency": issue_data["urgency"],
            "commands": [
                "kubectl get pods -A | grep ImagePull",
                "kubectl describe pod <pod-name>",
                "kubectl get secrets | grep docker"
            ]
        })
    
    # OOMKilled Analysis
    elif any(word in query_lower for word in ["oom", "memory", "killed", "out of memory"]):
        issue_data = EXPERT_KNOWLEDGE["pod_issues"]["oomkilled"]
        analysis_result.update({
            "issue_type": "oomkilled",
            "expert_analysis": issue_data["analysis"],
            "solutions": issue_data["solutions"],
            "confidence": issue_data["confidence"],
            "urgency": issue_data["urgency"],
            "commands": [
                "kubectl top pods -A --sort-by=memory",
                "kubectl describe pod <pod-name>",
                "kubectl get events --field-selector reason=OOMKilling"
            ]
        })
    
    # Node Issues
    elif any(word in query_lower for word in ["node", "notready", "ready false", "kubelet"]):
        issue_data = EXPERT_KNOWLEDGE["node_issues"]["notready"]
        analysis_result.update({
            "issue_type": "node_notready",
            "expert_analysis": issue_data["analysis"],
            "solutions": issue_data["solutions"],
            "confidence": issue_data["confidence"],
            "urgency": issue_data["urgency"],
            "commands": [
                "kubectl get nodes",
                "kubectl describe node <node-name>",
                "kubectl get events --field-selector involvedObject.kind=Node"
            ]
        })
    
    # Service Issues
    elif any(word in query_lower for word in ["service", "endpoint", "connection refused", "503", "502"]):
        issue_data = EXPERT_KNOWLEDGE["service_issues"]["no_endpoints"]
        analysis_result.update({
            "issue_type": "service_issue",
            "expert_analysis": issue_data["analysis"],
            "solutions": issue_data["solutions"],
            "confidence": issue_data["confidence"],
            "urgency": issue_data["urgency"],
            "commands": [
                "kubectl get svc,ep",
                "kubectl describe svc <service-name>",
                "kubectl get pods --show-labels"
            ]
        })
    
    # Generic troubleshooting
    else:
        analysis_result.update({
            "issue_type": "general",
            "expert_analysis": "üîç **General Kubernetes Issue Detected** - Applying systematic troubleshooting approach",
            "confidence": 0.6,
            "urgency": "MEDIUM",
            "solutions": [
                "üîç **Discovery**: Run `kubectl get all -A` to see cluster state",
                "üìã **Events**: Check `kubectl get events --sort-by='.lastTimestamp'`",
                "üìä **Resources**: Monitor with `kubectl top nodes` and `kubectl top pods -A`",
                "üè∑Ô∏è **Labels**: Verify selectors and labels match",
                "üåê **Networking**: Test service connectivity and DNS resolution"
            ],
            "commands": [
                "kubectl get all -A",
                "kubectl get events --sort-by='.lastTimestamp'",
                "kubectl top nodes",
                "kubectl top pods -A"
            ]
        })
    
    return analysis_result

def main():
    """Main interactive dashboard"""
    
    # Header with expert branding
    st.title("ü§ñ Kubernetes AI Agent - Interactive Expert Assistant")
    st.caption("üí¨ **Chat with your AI expert** ‚Ä¢ üîç **Real-time analysis** ‚Ä¢ üõ†Ô∏è **Automated fixes**")
    
    # Initialize components
    if K8S_COMPONENTS_AVAILABLE:
        init_success = initialize_components()
        if init_success:
            st.success("‚úÖ **AI Agent Online** - Kubernetes cluster connected")
        else:
            st.warning("‚ö†Ô∏è **Limited Mode** - Cluster access restricted")
    else:
        st.info("üí° **Demo Mode** - Interactive AI available, cluster monitoring disabled")
    
    # Sidebar controls
    with st.sidebar:
        st.header("üéõÔ∏è **AI Assistant Controls**")
        
        # Expert mode toggle
        st.session_state.expert_mode = st.checkbox("üß† Expert Mode", value=True, 
                                                  help="Enable advanced AI analysis")
        
        # Auto-refresh
        auto_refresh = st.checkbox("üîÑ Auto-refresh (30s)", value=False)
        
        # Quick actions
        st.subheader("‚ö° **Quick Actions**")
        if st.button("üîç **Scan Cluster**"):
            if st.session_state.monitor:
                with st.spinner("Scanning cluster for issues..."):
                    issues = st.session_state.monitor.scan_for_issues()
                    st.session_state.last_health_check = {
                        "issues": issues,
                        "timestamp": datetime.now(),
                        "total_issues": len(issues)
                    }
                    st.success(f"Found {len(issues)} issues")
        
        if st.button("üßπ **Clear Chat**"):
            st.session_state.ai_chat_history = []
            st.rerun()
    
    # Main chat interface - The core interactive feature
    st.header("üí¨ **Chat with Your Kubernetes Expert**")
    st.write("Ask me anything about your Kubernetes cluster - I'll provide expert analysis and solutions!")
    
    # Display chat history
    chat_container = st.container()
    with chat_container:
        for i, chat in enumerate(st.session_state.ai_chat_history):
            # User message
            st.markdown(f"""
            <div style="background-color: #e3f2fd; padding: 10px; border-radius: 5px; margin: 5px 0;">
                <strong>üßë‚Äçüíª You:</strong> {chat['query']}
            </div>
            """, unsafe_allow_html=True)
            
            # AI response with expert styling
            confidence_color = "üü¢" if chat.get('confidence', 0) > 0.8 else "üü°" if chat.get('confidence', 0) > 0.6 else "üî¥"
            urgency_color = {"CRITICAL": "üî¥", "HIGH": "üü†", "MEDIUM": "üü°", "LOW": "üü¢"}.get(chat.get('urgency', 'LOW'), "üü¢")
            
            st.markdown(f"""
            <div style="background-color: #f3e5f5; padding: 15px; border-radius: 5px; margin: 5px 0;">
                <strong>ü§ñ AI Expert:</strong><br>
                <strong>Issue Type:</strong> {chat.get('issue_type', 'unknown').title()}<br>
                <strong>Confidence:</strong> {confidence_color} {chat.get('confidence', 0):.0%} | 
                <strong>Urgency:</strong> {urgency_color} {chat.get('urgency', 'LOW')}<br><br>
                {chat.get('expert_analysis', 'Analysis not available')}
            </div>
            """, unsafe_allow_html=True)
            
            # Solutions
            if chat.get('solutions'):
                st.markdown("**üõ†Ô∏è Expert Solutions:**")
                for j, solution in enumerate(chat['solutions'], 1):
                    st.markdown(f"{j}. {solution}")
            
            # Diagnostic commands
            if chat.get('commands'):
                with st.expander("üìã **Diagnostic Commands**"):
                    for cmd in chat['commands']:
                        st.code(cmd)
            
            st.divider()
    
    # Chat input
    chat_input = st.text_area(
        "üí≠ **Describe your issue or ask a question:**",
        placeholder="e.g., 'My pod keeps crashing' or 'Node is not ready' or 'Service returning 503 errors'",
        height=100,
        key="chat_input"
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button("üöÄ **Ask AI Expert**", disabled=not chat_input.strip(), type="primary"):
            if chat_input.strip():
                with st.spinner("üß† AI Expert analyzing your issue..."):
                    analysis = expert_ai_analysis(chat_input.strip())
                    
                    # Add to chat history
                    st.session_state.ai_chat_history.append({
                        **analysis,
                        "timestamp": datetime.now()
                    })
                    
                    st.rerun()
    
    with col2:
        if st.button("üîß **Auto-Fix**"):
            if st.session_state.remediation and chat_input.strip():
                st.info("üöß Auto-remediation feature coming soon!")
    
    with col3:
        if st.button("üìä **Health Check**"):
            if st.session_state.monitor:
                with st.spinner("Running health check..."):
                    try:
                        health = st.session_state.monitor.run_health_check()
                        st.json(health)
                    except Exception as e:
                        st.error(f"Health check failed: {e}")
    
    # Quick troubleshooting section
    st.header("‚ö° **Quick Expert Analysis**")
    
    quick_issues = [
        "Pod in CrashLoopBackOff",
        "Pod stuck in Pending state", 
        "ImagePullBackOff error",
        "Node NotReady",
        "Service returning 503",
        "High memory usage",
        "Disk pressure on node"
    ]
    
    cols = st.columns(4)
    for i, issue in enumerate(quick_issues):
        with cols[i % 4]:
            if st.button(f"üîç {issue}", key=f"quick_{i}"):
                analysis = expert_ai_analysis(issue)
                st.session_state.ai_chat_history.append({
                    **analysis,
                    "timestamp": datetime.now()
                })
                st.rerun()
    
    # Status footer
    st.divider()
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ü§ñ AI Status", "üü¢ Online" if st.session_state.expert_mode else "‚ö†Ô∏è Basic")
    
    with col2:
        st.metric("üí¨ Chat History", len(st.session_state.ai_chat_history))
    
    with col3:
        st.metric("üïê Last Update", datetime.now().strftime("%H:%M:%S"))
    
    # Auto-refresh
    if auto_refresh:
        time.sleep(30)
        st.rerun()

if __name__ == "__main__":
    main()
        def text_area(self, *args, **kwargs): pass
        def metric(self, label, value): print(f"METRIC {label}: {value}")
        def tabs(self, tabs): return [self] * len(tabs)
        def sidebar(self): return self
        def container(self): return self
        def rerun(self): pass
        def plotly_chart(self, *args, **kwargs): pass
        def download_button(self, *args, **kwargs): pass
        def json(self, data): print(f"JSON: {data}")
        def code(self, text): print(f"CODE: {text}")
        def set_page_config(self, **kwargs): pass
        def status(self, label, **kwargs): return self
        def progress(self, value): pass
        def write_stream(self, generator): 
            for chunk in generator:
                print(f"STREAM: {chunk}")
        def select_slider(self, label, options, **kwargs): return options[0] if options else None
        def radio(self, label, options, **kwargs): return options[0] if options else None
        def multiselect(self, label, options, **kwargs): return []
        def slider(self, label, min_value=0, max_value=100, **kwargs): return min_value
        def toggle(self, label, **kwargs): return False
        def __enter__(self): return self
        def __exit__(self, *args): pass
    
    st = MockStreamlit()

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Pandas not available")
    PANDAS_AVAILABLE = False
    pd = None

try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Plotly not available")
    PLOTLY_AVAILABLE = False
    px = None
    go = None
from datetime import datetime, timedelta
import json
import sys
import os
import logging
import time
import re

# Configure logging to suppress warnings during import
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)
logging.getLogger("torch").setLevel(logging.WARNING)

# Add parent directory to path to import our modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import with error handling for offline operation
try:
    from agent.monitor import KubernetesMonitor
except ImportError as e:
    st.error(f"Failed to import KubernetesMonitor: {e}")
    KubernetesMonitor = None

try:
    from agent.rag_agent import RAGAgent
except ImportError as e:
    st.error(f"Failed to import RAGAgent: {e}")
    RAGAgent = None

try:
    from agent.remediate import RemediationEngine
except ImportError as e:
    st.warning(f"Failed to import RemediationEngine: {e}")
    RemediationEngine = None

try:
    from scheduler.forecast import ResourceForecaster
except ImportError as e:
    st.warning(f"Failed to import ResourceForecaster: {e}")
    ResourceForecaster = None

try:
    from glusterfs.analyze import GlusterFSAnalyzer
except ImportError as e:
    st.warning(f"Failed to import GlusterFSAnalyzer: {e}")
    GlusterFSAnalyzer = None

# Mock classes for when components are unavailable
class AdvancedRAGAgent:
    """Advanced RAG agent with latest LLM capabilities."""
    def __init__(self):
        self.llama_available = False
        self.offline_mode = True
        self.model_name = "llama-3.1-8b-instruct"  # Latest model
        self.context_window = 32768  # Extended context
        self.streaming_enabled = True
        self.function_calling_enabled = True
        self.conversation_memory = []
        
        # Model performance metrics
        self.response_time = 0
        self.tokens_generated = 0
        self.memory_usage = 0
        
        # Available models (latest and greatest)
        self.available_models = {
            "llama-3.1-8b-instruct": {"context": 32768, "speed": "fast", "quality": "high"},
            "llama-3.1-70b-instruct": {"context": 32768, "speed": "medium", "quality": "very_high"},
            "mistral-7b-instruct-v0.3": {"context": 32768, "speed": "fast", "quality": "high"},
            "codellama-34b-instruct": {"context": 16384, "speed": "medium", "quality": "high", "specialty": "code"},
            "vicuna-13b-v1.5": {"context": 4096, "speed": "fast", "quality": "medium"},
            "wizardlm-13b-v1.2": {"context": 4096, "speed": "fast", "quality": "high"},
            "openchat-3.5": {"context": 8192, "speed": "fast", "quality": "high"},
            "neural-chat-7b-v3": {"context": 8192, "speed": "fast", "quality": "medium"}
        }
        
        # System prompt for enhanced capabilities
        self.system_prompt = """You are an advanced Kubernetes AI Assistant with expert-level knowledge in:
        - Kubernetes operations, troubleshooting, and best practices
        - Container orchestration and cloud-native technologies
        - DevOps, CI/CD, and infrastructure automation
        - Performance optimization and security
        - Multi-cloud and hybrid deployments

        You can execute actions by providing structured responses. Available actions:
        - restart_pods: Restart failed or problematic pods
        - scale_deployment: Scale deployments up or down
        - clean_jobs: Clean completed or failed jobs
        - drain_node: Safely drain nodes for maintenance
        - apply_manifest: Apply Kubernetes manifests
        - run_kubectl: Execute kubectl commands

        Always provide:
        1. Clear, actionable guidance
        2. Relevant kubectl commands
        3. Security considerations
        4. Best practices
        5. Preventive measures

        Format responses with proper markdown and code blocks for better readability.
        """
    
    def switch_model(self, model_name: str):
        """Switch to a different LLM model."""
        if model_name in self.available_models:
            self.model_name = model_name
            model_info = self.available_models[model_name]
            self.context_window = model_info["context"]
            return f"‚úÖ Switched to {model_name} (Context: {self.context_window} tokens, Speed: {model_info['speed']}, Quality: {model_info['quality']})"
        else:
            return f"‚ùå Model {model_name} not available"
    
    def get_model_info(self):
        """Get current model information."""
        if self.model_name in self.available_models:
            info = self.available_models[self.model_name]
            return {
                "name": self.model_name,
                "context_window": self.context_window,
                "speed": info.get("speed", "unknown"),
                "quality": info.get("quality", "unknown"),
                "specialty": info.get("specialty", "general"),
                "streaming": self.streaming_enabled,
                "function_calling": self.function_calling_enabled
            }
        return {"name": "unknown", "context_window": 0}
    
    def query_stream(self, question: str, include_context: bool = True):
        """Stream response generation for real-time feedback."""
        import time
        
        # Simulate streaming response with realistic chunks
        response_chunks = [
            "Based on your Kubernetes question, ",
            "let me provide a comprehensive analysis.\n\n",
            "**Current Situation Analysis:**\n",
            f"- Query: {question[:50]}{'...' if len(question) > 50 else ''}\n",
            "- System: Operating in offline mode\n",
            "- Context: Full cluster visibility available\n\n",
            "**Recommended Actions:**\n",
            "1. **Immediate Steps:**\n",
            "   - Check resource status: `kubectl get pods,nodes,services`\n",
            "   - Review recent events: `kubectl get events --sort-by=.metadata.creationTimestamp`\n\n",
            "2. **Diagnostic Commands:**\n",
            "   ```bash\n",
            "   kubectl describe pods --all-namespaces\n",
            "   kubectl top nodes\n",
            "   kubectl get componentstatuses\n",
            "   ```\n\n",
            "3. **Monitoring & Logs:**\n",
            "   - Check system logs: `journalctl -u kubelet`\n",
            "   - Monitor resource usage: `kubectl top pods --all-namespaces`\n\n",
            "**Best Practices:**\n",
            "- Always backup before making changes\n",
            "- Use namespace isolation\n",
            "- Implement proper RBAC\n",
            "- Monitor resource quotas\n\n",
            "**Security Considerations:**\n",
            "- Verify pod security contexts\n",
            "- Check network policies\n",
            "- Review admission controllers\n\n",
            f"**Note:** This is an enhanced AI response using {self.model_name} ",
            "with extended context window and improved reasoning capabilities."
        ]
        
        for i, chunk in enumerate(response_chunks):
            time.sleep(0.1)  # Simulate processing time
            yield chunk
            self.tokens_generated += len(chunk.split())
    
    def query(self, question: str) -> str:
        """Enhanced query with improved reasoning and context."""
        start_time = time.time()
        
        # Add to conversation memory
        self.conversation_memory.append({"role": "user", "content": question})
        
        # Generate enhanced response
        response = f"""
        ü§ñ **Advanced AI Analysis** (Model: {self.model_name})
        
        **Your Question:** "{question}"
        
        ## üîç **Intelligent Analysis**
        
        Based on advanced reasoning and Kubernetes expertise, here's my analysis:
        
        ### **Context-Aware Response:**
        
        **1. Problem Identification:**
        - Analyzing query patterns and intent
        - Cross-referencing with Kubernetes best practices
        - Considering cluster-specific context
        
        **2. Strategic Recommendations:**
        ```bash
        # Advanced diagnostics
        kubectl get pods --all-namespaces --field-selector=status.phase!=Running
        kubectl get events --sort-by=.metadata.creationTimestamp --field-selector type=Warning
        kubectl describe nodes | grep -A 5 "Conditions"
        
        # Performance analysis
        kubectl top nodes --sort-by=cpu
        kubectl top pods --all-namespaces --sort-by=cpu
        
        # Security audit
        kubectl auth can-i --list --as=system:serviceaccount:default:default
        kubectl get networkpolicies --all-namespaces
        ```
        
        **3. Intelligent Automation:**
        - Proactive monitoring setup
        - Automated healing strategies
        - Performance optimization recommendations
        
        ### **üéØ Advanced Features:**
        
        **Multi-Modal Analysis:**
        - Text processing with enhanced NLP
        - Pattern recognition across logs
        - Predictive issue detection
        
        **Function Calling:**
        - Automatic action execution
        - Context-aware command generation
        - Safe execution with rollback capabilities
        
        **Extended Context Window:**
        - Full conversation history retention
        - Complex query understanding
        - Long-form troubleshooting guides
        
        ### **üìä Performance Metrics:**
        - **Model:** {self.model_name}
        - **Context Window:** {self.context_window:,} tokens
        - **Processing Mode:** {('Streaming' if self.streaming_enabled else 'Batch')}
        - **Function Calling:** {('Enabled' if self.function_calling_enabled else 'Disabled')}
        
        ### **üîí Security & Compliance:**
        - All processing happens locally
        - No data transmitted externally
        - RBAC-compliant operations
        - Audit trail maintained
        
        ### **üí° Proactive Suggestions:**
        
        Based on the latest Kubernetes trends and best practices:
        
        1. **GitOps Integration:** Consider implementing ArgoCD or Flux
        2. **Observability Stack:** Deploy Prometheus + Grafana + Jaeger
        3. **Security Hardening:** Implement Pod Security Standards
        4. **Backup Strategy:** Set up Velero for disaster recovery
        5. **Cost Optimization:** Use Vertical Pod Autoscaler (VPA)
        
        ---
        
        **üöÄ Ready for Action:** Use the quick action buttons above or ask me to execute any of these recommendations!
        """
        
        # Update metrics
        self.response_time = time.time() - start_time
        self.tokens_generated += len(response.split())
        
        # Add to conversation memory
        self.conversation_memory.append({"role": "assistant", "content": response})
        
        # Trim memory if too long
        if len(self.conversation_memory) > 20:
            self.conversation_memory = self.conversation_memory[-20:]
        
        return response
    
    def query_with_actions(self, question: str, remediation_engine=None):
        """Enhanced query with advanced action detection and execution."""
        # Use expert query if available
        if hasattr(self, 'expert_query') and callable(getattr(self, 'expert_query')):
            try:
                expert_result = self.expert_query(question, auto_remediate=False)
                return {
                    "query_response": expert_result.get("expert_response", expert_result.get("standard_response", "")),
                    "action_result": {
                        "action_detected": expert_result.get("confidence", 0.0) > 0.5,
                        "action_type": expert_result.get("expert_analysis", {}).get("issue_analysis", {}).get("issue_type", "unknown"),
                        "executed": len(expert_result.get("expert_analysis", {}).get("executed_actions", [])) > 0,
                        "confidence": expert_result.get("confidence", 0.0),
                        "message": f"Expert analysis confidence: {expert_result.get('confidence', 0.0):.1%}",
                        "available_actions": ["analyze", "remediate", "monitor", "verify"]
                    },
                    "full_response": expert_result.get("expert_response", expert_result.get("standard_response", "")),
                    "model_info": self.get_model_info(),
                    "conversation_length": len(self.conversation_memory),
                    "expert_analysis": expert_result.get("expert_analysis", {}),
                    "system_health": expert_result.get("system_health", {})
                }
            except Exception as e:
                # Fallback to standard method
                pass
        
        # Advanced action detection using NLP patterns
        action_patterns = {
            "restart": {
                "patterns": [r"restart.*pod", r"reboot.*pod", r"bounce.*pod", r"cycle.*pod"],
                "action_type": "restart_pods",
                "confidence_threshold": 0.7
            },
            "scale": {
                "patterns": [r"scale.*(\d+)", r"increase.*replica", r"decrease.*replica", r"replicas?.*(\d+)"],
                "action_type": "scale_deployment", 
                "confidence_threshold": 0.8
            },
            "clean": {
                "patterns": [r"clean.*job", r"remove.*completed", r"delete.*finished", r"cleanup"],
                "action_type": "clean_jobs",
                "confidence_threshold": 0.6
            },
            "deploy": {
                "patterns": [r"deploy.*", r"apply.*manifest", r"create.*resource"],
                "action_type": "apply_manifest",
                "confidence_threshold": 0.7
            },
            "analyze": {
                "patterns": [r"analyze.*", r"check.*health", r"diagnose.*", r"investigate.*"],
                "action_type": "system_analysis",
                "confidence_threshold": 0.6
            },
            "fix": {
                "patterns": [r"fix.*", r"resolve.*", r"remediate.*", r"solve.*"],
                "action_type": "auto_remediate",
                "confidence_threshold": 0.8
            }
        }
        
        # Analyze intent with confidence scoring
        detected_actions = []
        for action_name, config in action_patterns.items():
            for pattern in config["patterns"]:
                if re.search(pattern, question.lower()):
                    confidence = min(1.0, len(re.findall(pattern, question.lower())) * 0.3 + 0.6)
                    if confidence >= config["confidence_threshold"]:
                        detected_actions.append({
                            "action": config["action_type"],
                            "confidence": confidence,
                            "trigger_phrase": pattern
                        })
        
        # Execute highest confidence action
        action_result = {
            "action_detected": len(detected_actions) > 0,
            "action_type": None,
            "executed": False,
            "message": "No actions available in demo mode",
            "confidence": 0,
            "available_actions": list(action_patterns.keys())
        }
        
        if detected_actions:
            best_action = max(detected_actions, key=lambda x: x["confidence"])
            action_result.update({
                "action_type": best_action["action"],
                "confidence": best_action["confidence"],
                "trigger_phrase": best_action["trigger_phrase"],
                "message": f"Detected action: {best_action['action']} (confidence: {best_action['confidence']:.2f})"
            })
            
            # Mock execution for demo
            if remediation_engine:
                action_result["executed"] = True
                action_result["message"] = f"‚úÖ Successfully executed {best_action['action']} with {best_action['confidence']:.0%} confidence"
        
        # Generate comprehensive response
        base_response = self.query(question)
        
        # Add action context if action detected
        if action_result["action_detected"]:
            action_context = f"""
            
            ## üéØ **Action Intelligence**
            
            **Detected Intent:** {action_result["action_type"]}
            **Confidence Score:** {action_result["confidence"]:.1%}
            **Execution Status:** {'‚úÖ Completed' if action_result["executed"] else '‚è≥ Pending'}
            
            **Available Actions:**
            {' | '.join([f"`{action}`" for action in action_result["available_actions"]])}
            
            **Smart Recommendations:**
            - Use natural language: "restart the failing pods"
            - Be specific: "scale nginx deployment to 5 replicas" 
            - Combine actions: "clean jobs and restart failed pods"
            """
            base_response += action_context
        
        return {
            "query_response": base_response,
            "action_result": action_result,
            "full_response": base_response,
            "model_info": self.get_model_info(),
            "conversation_length": len(self.conversation_memory)
        }
    
    def get_conversation_summary(self):
        """Get conversation summary with advanced analytics."""
        if not self.conversation_memory:
            return "No conversation history"
        
        user_messages = [msg for msg in self.conversation_memory if msg["role"] == "user"]
        assistant_messages = [msg for msg in self.conversation_memory if msg["role"] == "assistant"]
        
        return {
            "total_exchanges": len(user_messages),
            "avg_response_time": self.response_time,
            "total_tokens": self.tokens_generated,
            "memory_usage": len(str(self.conversation_memory)),
            "active_model": self.model_name,
            "recent_topics": [msg["content"][:50] + "..." for msg in user_messages[-3:]]
        }

class MockRAGAgent(AdvancedRAGAgent):
    """Enhanced mock RAG agent inheriting advanced features."""
    pass

class MockRemediationEngine:
    """Mock remediation engine for demo purposes."""
    def restart_failed_pods(self):
        return {"count": 0, "success": False, "message": "Mock mode - no actions performed"}
    
    def clean_completed_jobs(self):
        return {"count": 0, "success": False, "message": "Mock mode - no actions performed"}
    
    def clean_orphaned_storage(self):
        return {"count": 0, "success": False, "message": "Mock mode - no actions performed"}

class MockForecaster:
    """Mock forecaster for demo purposes."""
    def get_latest_forecast(self):
        return None
    
    def get_placement_recommendations(self):
        return []
    
    def generate_forecast(self, days, resource_type):
        pass

class MockGlusterFSAnalyzer:
    """Mock GlusterFS analyzer for demo purposes."""
    def get_health_status(self):
        return {
            "volumes_healthy": "Unknown",
            "peers_connected": "Unknown", 
            "heal_pending": 0,
            "split_brain_files": 0
        }
    
    def get_heal_map(self):
        return []
    
    def analyze_peers(self):
        return []

st.set_page_config(
    page_title="K8s AI Assistant",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state with offline mode and error handling
if 'chat_messages' not in st.session_state:
    st.session_state.chat_messages = []

if 'monitor' not in st.session_state:
    if KubernetesMonitor:
        try:
            st.session_state.monitor = KubernetesMonitor()
        except Exception as e:
            st.error(f"Failed to initialize Kubernetes monitor: {e}")
            st.session_state.monitor = None
    else:
        st.session_state.monitor = None

if 'rag_agent' not in st.session_state:
    if RAGAgent:
        try:
            st.session_state.rag_agent = RAGAgent(offline_mode=True)
        except Exception as e:
            st.error(f"Failed to initialize RAG agent: {e}")
            # Create an advanced mock RAG agent for demo purposes
            st.session_state.rag_agent = AdvancedRAGAgent()
    else:
        st.session_state.rag_agent = AdvancedRAGAgent()

if 'remediation' not in st.session_state:
    if RemediationEngine:
        try:
            st.session_state.remediation = RemediationEngine()
        except Exception as e:
            st.warning(f"Remediation engine not available: {e}")
            st.session_state.remediation = MockRemediationEngine()
    else:
        st.session_state.remediation = MockRemediationEngine()

if 'forecaster' not in st.session_state:
    if ResourceForecaster:
        try:
            st.session_state.forecaster = ResourceForecaster()
        except Exception as e:
            st.warning(f"Forecaster not available: {e}")
            st.session_state.forecaster = MockForecaster()
    else:
        st.session_state.forecaster = MockForecaster()

if 'glusterfs' not in st.session_state:
    if GlusterFSAnalyzer:
        try:
            st.session_state.glusterfs = GlusterFSAnalyzer()
        except Exception as e:
            st.warning(f"GlusterFS analyzer not available: {e}")
            st.session_state.glusterfs = MockGlusterFSAnalyzer()
    else:
        st.session_state.glusterfs = MockGlusterFSAnalyzer()

def main():
    # Initialize configuration manager
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'agent'))
        from runtime_config_manager import get_config_manager, OperationalMode
        config_manager = get_config_manager()
    except ImportError:
        try:
            from config_manager import get_config_manager, OperationalMode
            config_manager = get_config_manager()
        except ImportError:
            config_manager = None
            OperationalMode = None
    
    # Dashboard title with mode indicator
    if config_manager:
        mode_emoji = {
            'debug': 'üîç',
            'remediation': 'üîß', 
            'interactive': 'üí¨',
            'monitoring': 'üìä',
            'hybrid': 'üîÑ'
        }
        current_mode = config_manager.current_mode.value
        mode_icon = mode_emoji.get(current_mode, 'ü§ñ')
        st.title(f"{mode_icon} Kubernetes AI Assistant Dashboard - {current_mode.title()} Mode")
        
        # Show mode description
        st.caption(f"üéõÔ∏è {config_manager.get_mode_description()}")
        st.caption(f"ü§ñ {config_manager.get_automation_description()}")
    else:
        st.title("üöÄ Kubernetes AI Assistant Dashboard")
    
    st.caption("üîí **Offline Mode**: All operations performed locally within your cluster")
    
    # Component status indicators
    with st.sidebar:
        st.header("üéõÔ∏è Control Panel")
        
        # Mode Configuration Section
        if config_manager:
            st.subheader("üéØ Operational Mode")
            
            # Current mode display
            current_status = config_manager.get_status_summary()
            mode_name = current_status['operational_mode']
            automation_name = current_status['automation_level']
            
            st.info(f"**Mode**: {mode_name.title()}")
            st.info(f"**Automation**: {automation_name.replace('_', ' ').title()}")
            
            # Mode capabilities
            with st.expander("üîß Mode Capabilities"):
                runtime_checks = current_status['runtime_checks']
                for key, value in runtime_checks.items():
                    emoji = "‚úÖ" if value else "‚ùå"
                    label = key.replace('_', ' ').title()
                    st.write(f"{emoji} {label}")
            
            # Quick mode switch (for demo purposes)
            with st.expander("‚öôÔ∏è Mode Settings"):
                if st.button("üîç Switch to Debug Mode"):
                    try:
                        config_manager.set_mode(OperationalMode.DEBUG)
                        st.success("Switched to Debug Mode")
                        st.rerun()
                    except:
                        st.error("Failed to switch mode")
                
                if st.button("üîß Switch to Remediation Mode"):
                    try:
                        config_manager.set_mode(OperationalMode.REMEDIATION)
                        st.success("Switched to Remediation Mode")
                        st.rerun()
                    except:
                        st.error("Failed to switch mode")
                
                if st.button("üí¨ Switch to Interactive Mode"):
                    try:
                        config_manager.set_mode(OperationalMode.INTERACTIVE)
                        st.success("Switched to Interactive Mode") 
                        st.rerun()
                    except:
                        st.error("Failed to switch mode")
        
        st.divider()
        
        # System Status
        st.subheader("System Status")
        
        # Cluster Connection Status
        if st.session_state.monitor:
            try:
                if st.session_state.monitor.is_connected():
                    st.success("‚úÖ Kubernetes API")
                else:
                    st.error("‚ùå Kubernetes API")
            except:
                st.warning("‚ö†Ô∏è Kubernetes API")
        else:
            st.error("‚ùå Kubernetes Monitor")
        
        # RAG Agent Status
        if st.session_state.rag_agent:
            if st.session_state.rag_agent.llama_available:
                st.success("‚úÖ LLM Engine")
            else:
                st.info("üîß LLM (Offline Mode)")
        else:
            st.error("‚ùå RAG Agent")
        
        # Component availability
        st.write("**Components:**")
        st.write(f"‚Ä¢ Remediation: {'‚úÖ' if st.session_state.remediation else '‚ùå'}")
        
        # Mode-specific status
        if config_manager:
            config = config_manager.get_current_config()
            st.write("**Mode Features:**")
            st.write(f"‚Ä¢ Auto-Remediation: {'‚úÖ' if config.auto_remediation else '‚ùå'}")
            st.write(f"‚Ä¢ Historical Learning: {'‚úÖ' if config.historical_learning else '‚ùå'}")
            st.write(f"‚Ä¢ Predictive Analysis: {'‚úÖ' if config.predictive_analysis else '‚ùå'}")
            st.write(f"‚Ä¢ Continuous Monitoring: {'‚úÖ' if config.continuous_monitoring else '‚ùå'}")
        st.write(f"‚Ä¢ Forecasting: {'‚úÖ' if st.session_state.forecaster else '‚ùå'}")
        st.write(f"‚Ä¢ GlusterFS: {'‚úÖ' if st.session_state.glusterfs else '‚ùå'}")
        
        st.divider()
        
        # Quick Actions
        st.subheader("Quick Actions")
        if st.button("üîç Scan for Issues"):
            scan_for_issues()
        if st.button("üìä Generate Report"):
            generate_report()
        if st.button("üè• Health Check"):
            run_health_check()
    
    # Main tabs - Mode-aware interface
    if config_manager:
        mode = config_manager.current_mode.value
        if mode == 'debug':
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üîç Debug Analysis", 
                "üí¨ Chat Assistant", 
                "üìã Logs & Issues", 
                "üìà Forecasting", 
                "üß† Learning Insights"
            ])
        elif mode == 'remediation':
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üîß Auto Remediation", 
                "üí¨ Chat Assistant", 
                "üìã System Status", 
                "üìà Forecasting", 
                "üóÑÔ∏è GlusterFS Health"
            ])
        elif mode == 'monitoring':
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üìä Real-time Monitor", 
                "üí¨ Chat Assistant", 
                "üö® Alert Dashboard", 
                "üìà Forecasting", 
                "üîç System Analysis"
            ])
        else:  # interactive or hybrid
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üí¨ Chat Assistant", 
                "üìã Logs & Issues", 
                "üìà Forecasting", 
                "üóÑÔ∏è GlusterFS Health",
                "‚öôÔ∏è Manual Remediation"
            ])
    else:
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üí¨ Chat Assistant", 
            "üìã Logs & Issues", 
            "üìà Forecasting", 
            "üóÑÔ∏è GlusterFS Health",
            "‚öôÔ∏è Manual Remediation"
        ])
    
    # Tab content based on mode
    with tab1:
        if config_manager:
            mode = config_manager.current_mode.value
            config = config_manager.get_current_config()
            
            if mode == 'debug':
                st.header("üîç Debug Mode - Root Cause Analysis")
                st.info("üéØ Focus: Deep analysis and diagnosis without automatic remediation")
                debug_analysis_interface(config_manager)
            elif mode == 'remediation':
                st.header("üîß Auto Remediation Mode")
                st.info("‚ö° Automatic issue resolution based on confidence thresholds")
                auto_remediation_interface(config_manager)
            elif mode == 'monitoring':
                st.header("üìä Real-time Monitoring Dashboard")
                st.info("üîÑ Continuous cluster monitoring with instant alerts")
                monitoring_interface(config_manager)
            else:
                if st.session_state.rag_agent:
                    chat_interface(config_manager)
                else:
                    st.error("‚ùå Chat interface not available - RAG agent failed to initialize")
        else:
            if st.session_state.rag_agent:
                chat_interface(config_manager)
            else:
                st.error("‚ùå Chat interface not available - RAG agent failed to initialize")
    
    with tab2:
        if st.session_state.rag_agent:
            chat_interface(config_manager)
        else:
            st.error("‚ùå Chat interface not available - RAG agent failed to initialize")
    
    with tab3:
        if config_manager and config_manager.current_mode.value == 'monitoring':
            alert_dashboard()
        else:
            logs_and_issues()
    
    with tab4:
        forecasting_dashboard()
    
    with tab5:
        if config_manager and config_manager.current_mode.value == 'debug':
            learning_insights_interface()
        elif config_manager and config_manager.current_mode.value == 'remediation':
            glusterfs_dashboard()
        elif config_manager and config_manager.current_mode.value == 'monitoring':
            system_analysis_interface()
        else:
            manual_remediation()

def chat_interface(config_manager=None):
    st.header("ü§ñ Advanced AI Chat Assistant - Latest LLM Technology")
    
    # Model selection and status
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        if hasattr(st.session_state.rag_agent, 'available_models'):
            current_model = st.selectbox(
                "üß† Select AI Model:",
                options=list(st.session_state.rag_agent.available_models.keys()),
                index=0,
                help="Choose from latest state-of-the-art language models"
            )
            if current_model != st.session_state.rag_agent.model_name:
                result = st.session_state.rag_agent.switch_model(current_model)
                st.success(result)
                st.rerun()
    
    with col2:
        if hasattr(st.session_state.rag_agent, 'get_model_info'):
            model_info = st.session_state.rag_agent.get_model_info()
            st.metric("Context Window", f"{model_info.get('context_window', 0):,}")
    
    with col3:
        if hasattr(st.session_state.rag_agent, 'streaming_enabled'):
            streaming = st.toggle("üîÑ Streaming", value=st.session_state.rag_agent.streaming_enabled)
            st.session_state.rag_agent.streaming_enabled = streaming
    
    # Advanced model capabilities display
    with st.expander("üöÄ Advanced AI Capabilities", expanded=False):
        col_a, col_b, col_c = st.columns(3)
        
        with col_a:
            st.write("**üß† Model Features:**")
            if hasattr(st.session_state.rag_agent, 'get_model_info'):
                info = st.session_state.rag_agent.get_model_info()
                st.write(f"‚Ä¢ Quality: {info.get('quality', 'N/A')}")
                st.write(f"‚Ä¢ Speed: {info.get('speed', 'N/A')}")
                st.write(f"‚Ä¢ Specialty: {info.get('specialty', 'General')}")
        
        with col_b:
            st.write("**‚ö° Performance:**")
            if hasattr(st.session_state.rag_agent, 'response_time'):
                st.write(f"‚Ä¢ Response Time: {st.session_state.rag_agent.response_time:.2f}s")
                st.write(f"‚Ä¢ Tokens Generated: {st.session_state.rag_agent.tokens_generated:,}")
                st.write(f"‚Ä¢ Function Calling: {'‚úÖ' if getattr(st.session_state.rag_agent, 'function_calling_enabled', False) else '‚ùå'}")
        
        with col_c:
            st.write("**üîç Analytics:**")
            if hasattr(st.session_state.rag_agent, 'get_conversation_summary'):
                summary = st.session_state.rag_agent.get_conversation_summary()
                st.write(f"‚Ä¢ Exchanges: {summary.get('total_exchanges', 0)}")
                st.write(f"‚Ä¢ Memory Usage: {summary.get('memory_usage', 0)} chars")
                st.write(f"‚Ä¢ Conversation Length: {getattr(st.session_state.rag_agent, 'conversation_memory', []) and len(st.session_state.rag_agent.conversation_memory) or 0}")
    
    # Enhanced status indicators
    status_col1, status_col2, status_col3, status_col4 = st.columns(4)
    with status_col1:
        llama_status = "üü¢ Online" if getattr(st.session_state.rag_agent, 'llama_available', False) else "üü° Offline"
        st.info(f"**LLM Status:** {llama_status}")
    
    with status_col2:
        mode = "üîí Offline" if getattr(st.session_state.rag_agent, 'offline_mode', True) else "üåê Online"
        st.info(f"**Processing:** {mode}")
    
    with status_col3:
        streaming_status = "üîÑ Enabled" if getattr(st.session_state.rag_agent, 'streaming_enabled', False) else "‚è∏Ô∏è Disabled"
        st.info(f"**Streaming:** {streaming_status}")
    
    with status_col4:
        context_size = getattr(st.session_state.rag_agent, 'context_window', 0)
        st.info(f"**Context:** {context_size:,} tokens")
    
    # AI-powered quick actions with confidence scoring
    st.subheader("üéØ Expert AI-Powered Actions")
    action_col1, action_col2, action_col3, action_col4, action_col5 = st.columns(5)
    
    with action_col1:
        if st.button("ÔøΩ Expert Diagnosis", help="AI performs comprehensive system analysis"):
            execute_expert_action("perform comprehensive expert diagnosis of the entire system including Ubuntu OS, Kubernetes, and GlusterFS")
    
    with action_col2:
        if st.button("üöÄ Auto-Remediate", help="AI automatically fixes detected issues"):
            execute_expert_action("automatically detect and remediate all critical system issues with safety checks")
    
    with action_col3:
        if st.button("ü©∫ Health Check", help="AI performs deep health analysis"):
            execute_expert_action("conduct deep health analysis across all system components and provide detailed report")
    
    with action_col4:
        if st.button("‚ö° Smart Optimize", help="AI suggests and applies performance optimizations"):
            execute_expert_action("analyze system performance and apply safe optimization recommendations")
    
    with action_col5:
        if st.button("üõ°Ô∏è Security Audit", help="AI performs comprehensive security assessment"):
            execute_expert_action("perform comprehensive security audit and hardening recommendations")
    
    # Legacy quick actions for backward compatibility
    st.subheader("üéØ AI-Powered Smart Actions")
    legacy_col1, legacy_col2, legacy_col3, legacy_col4, legacy_col5 = st.columns(5)
    
    with legacy_col1:
        if st.button("ÔøΩüîÑ Smart Restart", help="AI analyzes and restarts only problematic pods"):
            execute_smart_action("intelligently restart any failed or problematic pods")
    
    with legacy_col2:
        if st.button("üßπ Smart Cleanup", help="AI identifies and cleans safe-to-remove resources"):
            execute_smart_action("analyze and safely clean up completed jobs and unused resources")
    
    with legacy_col3:
        if st.button("üìä Health Analysis", help="AI performs comprehensive cluster health check"):
            execute_smart_action("perform a comprehensive health analysis of the entire cluster")
    
    with legacy_col4:
        if st.button("‚ö° Auto-Optimize", help="AI suggests performance optimizations"):
            execute_smart_action("analyze the cluster and suggest performance optimizations")
    
    with legacy_col5:
        if st.button("üîê Security Audit", help="AI performs security assessment"):
            execute_smart_action("conduct a security audit and identify potential vulnerabilities")
    
    st.divider()
    
    # Chat container with enhanced features
    chat_container = st.container()
    
    # Display chat messages with advanced formatting
    with chat_container:
        for i, message in enumerate(st.session_state.chat_messages):
            with st.chat_message(message["role"]):
                # Enhanced message display with metadata
                if message["role"] == "assistant" and hasattr(st.session_state.rag_agent, 'get_model_info'):
                    model_info = st.session_state.rag_agent.get_model_info()
                    st.caption(f"ü§ñ {model_info.get('name', 'AI Assistant')} ‚Ä¢ Quality: {model_info.get('quality', 'N/A')} ‚Ä¢ Context: {model_info.get('context_window', 0):,} tokens")
                
                st.markdown(message["content"])
                
                # Enhanced action results display
                if message.get("action_result") and message["action_result"]["action_detected"]:
                    with st.expander("üéØ AI Action Analysis", expanded=False):
                        action_result = message["action_result"]
                        
                        # Action confidence visualization
                        confidence = action_result.get("confidence", 0)
                        st.progress(confidence, text=f"Confidence: {confidence:.1%}")
                        
                        # Action details
                        col_x, col_y = st.columns(2)
                        with col_x:
                            st.write(f"**Action Type:** {action_result['action_type']}")
                            st.write(f"**Executed:** {'‚úÖ Yes' if action_result['executed'] else '‚ùå No'}")
                        with col_y:
                            st.write(f"**Trigger:** {action_result.get('trigger_phrase', 'N/A')}")
                            st.write(f"**Model:** {message.get('model_info', {}).get('name', 'Unknown')}")
                        
                        st.write(f"**Result:** {action_result['message']}")
                        
                        if action_result.get("result"):
                            st.json(action_result["result"])
                
                # Conversation analytics for assistant messages
                if message["role"] == "assistant" and message.get("model_info"):
                    with st.expander("üìä Response Analytics", expanded=False):
                        info = message["model_info"]
                        metrics_col1, metrics_col2 = st.columns(2)
                        with metrics_col1:
                            st.metric("Response Quality", info.get("quality", "N/A"))
                            st.metric("Processing Speed", info.get("speed", "N/A"))
                        with metrics_col2:
                            st.metric("Function Calling", "‚úÖ" if info.get("function_calling") else "‚ùå")
                            st.metric("Streaming Support", "‚úÖ" if info.get("streaming") else "‚ùå")
    
    # Enhanced chat input with AI suggestions
    st.subheader("ÔøΩ Advanced AI Conversation")
    
    # AI-powered suggestion tabs
    suggestion_tab1, suggestion_tab2, suggestion_tab3 = st.tabs(["üéØ Smart Queries", "‚ö° Quick Actions", "üîß Advanced Operations"])
    
    with suggestion_tab1:
        smart_queries = [
            "What Ubuntu system issues need immediate attention and how to fix them?",
            "Analyze Kubernetes cluster health and identify critical problems",
            "Check GlusterFS distributed storage for any split-brain or heal issues",
            "Perform comprehensive security audit across all system components",
            "What are the current performance bottlenecks and optimization opportunities?",
            "Identify any failed services, high resource usage, or network connectivity issues",
            "Check for disk space issues, memory pressure, or CPU overload conditions",
            "Analyze pod crashes, pending workloads, and service connectivity problems"
        ]
        for query in smart_queries:
            if st.button(f"üí° {query}", key=f"smart_{hash(query)}"):
                execute_suggested_expert_query(query)
    
    with suggestion_tab2:
        quick_actions = [
            "Automatically restart all failed pods and services across the system",
            "Clean up completed jobs, old logs, and unnecessary files to free space",
            "Scale deployments based on current load and resource availability", 
            "Fix any Ubuntu service failures and restart problematic components",
            "Resolve GlusterFS volume issues, heal operations, and peer connectivity",
            "Apply security patches and hardening recommendations",
            "Optimize system performance by adjusting resource limits and requests",
            "Perform emergency system recovery and stability restoration"
        ]
        for action in quick_actions:
            if st.button(f"‚ö° {action}", key=f"action_{hash(action)}"):
                execute_suggested_expert_query(action)
    
    with suggestion_tab3:
        advanced_ops = [
            "Generate comprehensive disaster recovery plan for the entire infrastructure",
            "Create automated monitoring and alerting for all critical system components",
            "Implement zero-downtime deployment strategy with rollback capabilities",
            "Setup multi-region failover and backup strategies for data protection",
            "Optimize resource allocation and implement intelligent autoscaling policies",
            "Conduct penetration testing and vulnerability assessment",
            "Implement advanced logging, monitoring, and observability stack",
            "Create automated remediation workflows for common system issues"
        ]
        for op in advanced_ops:
            if st.button(f"üîß {op}", key=f"advanced_{hash(op)}"):
                execute_suggested_expert_query(op)
    
    # Main chat input with streaming support
    if prompt := st.chat_input("Ask anything about your cluster or request intelligent actions..."):
        # Add user message
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        
        # Generate response with streaming if enabled
        with st.chat_message("assistant"):
            if getattr(st.session_state.rag_agent, 'streaming_enabled', False) and hasattr(st.session_state.rag_agent, 'query_stream'):
                # Streaming response
                with st.status("üß† AI Processing...") as status:
                    response_placeholder = st.empty()
                    full_response = ""
                    
                    try:
                        for chunk in st.session_state.rag_agent.query_stream(prompt):
                            full_response += chunk
                            response_placeholder.markdown(full_response + "‚ñä")
                        
                        response_placeholder.markdown(full_response)
                        status.update(label="‚úÖ Response Complete", state="complete")
                        
                        # Get action analysis with mode awareness
                        result = get_mode_aware_response(prompt, st.session_state.rag_agent, st.session_state.remediation, config_manager)
                        
                    except Exception as e:
                        st.error(f"Streaming error: {e}")
                        result = get_mode_aware_response(prompt, st.session_state.rag_agent, st.session_state.remediation, config_manager)
                        full_response = result["full_response"]
                        response_placeholder.markdown(full_response)
            else:
                # Standard response
                with st.spinner("üß† Advanced AI Processing..."):
                    result = get_mode_aware_response(prompt, st.session_state.rag_agent, st.session_state.remediation, config_manager)
                    full_response = result["full_response"]
                    
                    st.markdown(full_response)
            
            # Store enhanced message with metadata
            assistant_message = {
                "role": "assistant",
                "content": full_response,
                "action_result": result.get("action_result", {}),
                "model_info": result.get("model_info", {}),
                "timestamp": datetime.now().isoformat(),
                "conversation_id": len(st.session_state.chat_messages)
            }
            st.session_state.chat_messages.append(assistant_message)
            
            # Show action execution results with enhanced UI
            if result.get("action_result", {}).get("executed"):
                st.balloons()  # Celebration for successful actions
                st.success(f"üéØ Action completed: {result['action_result']['message']}")
                
                # Refresh relevant data
                if any(keyword in prompt.lower() for keyword in ['restart', 'clean', 'scale', 'deploy']):
                    st.rerun()
            elif result.get("action_result", {}).get("action_detected"):
                confidence = result["action_result"].get("confidence", 0)
                if confidence > 0.8:
                    st.info(f"üéØ High-confidence action detected: {result['action_result']['action_type']} ({confidence:.1%})")
                else:
                    st.warning(f"‚ö†Ô∏è Possible action detected: {result['action_result']['action_type']} ({confidence:.1%}) - Consider being more specific")
    
    # Conversation management
    if st.session_state.chat_messages:
        col_clear, col_export, col_summary = st.columns(3)
        
        with col_clear:
            if st.button("üóëÔ∏è Clear Conversation"):
                st.session_state.chat_messages = []
                if hasattr(st.session_state.rag_agent, 'conversation_memory'):
                    st.session_state.rag_agent.conversation_memory = []
                st.rerun()
        
        with col_export:
            if st.button("üìÑ Export Chat"):
                export_conversation()
        
        with col_summary:
            if st.button("üìä Conversation Analytics"):
                show_conversation_analytics()

def execute_expert_action(action_text):
    """Execute an expert-level AI action with comprehensive analysis."""
    with st.spinner(f"üß† Expert AI analyzing and executing: {action_text}..."):
        # Use expert query if available
        if hasattr(st.session_state.rag_agent, 'expert_query') and callable(getattr(st.session_state.rag_agent, 'expert_query')):
            try:
                result = st.session_state.rag_agent.expert_query(
                    action_text, 
                    auto_remediate=True
                )
                
                if result.get("expert_analysis", {}).get("executed_actions"):
                    st.success(f"‚úÖ Expert action completed successfully!")
                    
                    # Show detailed results
                    with st.expander("üìä Expert Analysis Results", expanded=True):
                        system_health = result.get("system_health", {})
                        overall_health = system_health.get("overall_health", "unknown")
                        
                        col_a, col_b, col_c = st.columns(3)
                        with col_a:
                            health_emoji = {"healthy": "üü¢", "warning": "üü°", "degraded": "üü†", "critical": "üî¥"}.get(overall_health, "‚ö™")
                            st.metric("System Health", f"{health_emoji} {overall_health.title()}")
                        
                        with col_b:
                            confidence = result.get("confidence", 0.0)
                            st.metric("Analysis Confidence", f"{confidence:.1%}")
                        
                        with col_c:
                            executed_count = len(result.get("expert_analysis", {}).get("executed_actions", []))
                            st.metric("Actions Executed", executed_count)
                        
                        # Show critical issues if any
                        critical_issues = system_health.get("critical_issues", [])
                        if critical_issues:
                            st.write("**üö® Critical Issues Addressed:**")
                            for issue in critical_issues[:3]:
                                st.write(f"‚Ä¢ {issue}")
                        
                        # Show remediation results
                        executed_actions = result.get("expert_analysis", {}).get("executed_actions", [])
                        if executed_actions:
                            st.write("**‚ö° Actions Performed:**")
                            for action in executed_actions:
                                status = "‚úÖ" if action.get("success") else "‚ùå"
                                st.write(f"{status} {action.get('description', 'Unknown action')}")
                else:
                    st.info(f"üîç Expert analysis completed. View detailed results below.")
                
                # Add to chat history with enhanced metadata
                st.session_state.chat_messages.append({
                    "role": "user", 
                    "content": action_text,
                    "action_type": "expert_action"
                })
                st.session_state.chat_messages.append({
                    "role": "assistant", 
                    "content": result.get("expert_response", result.get("standard_response", "")),
                    "expert_analysis": result.get("expert_analysis", {}),
                    "system_health": result.get("system_health", {}),
                    "confidence": result.get("confidence", 0.0),
                    "timestamp": datetime.now().isoformat(),
                    "action_type": "expert_response"
                })
                st.rerun()
                
            except Exception as e:
                st.error(f"‚ùå Expert action failed: {e}")
                # Fallback to standard action
                execute_smart_action(action_text)
        else:
            # Fallback to standard smart action
            st.warning("Expert agent not available, falling back to standard AI action")
            execute_smart_action(action_text)

def execute_smart_action(action_text):
    """Execute a smart AI-powered action with enhanced feedback."""
    with st.spinner(f"üß† AI analyzing and executing: {action_text}..."):
        result = st.session_state.rag_agent.query_with_actions(
            action_text, 
            st.session_state.remediation
        )
        
        if result["action_result"]["executed"]:
            st.success(f"‚úÖ {result['action_result']['message']}")
            
            # Show confidence and analytics
            confidence = result["action_result"].get("confidence", 0)
            if confidence > 0:
                st.info(f"üéØ Action executed with {confidence:.1%} confidence")
            
            # Add to chat history with enhanced metadata
            st.session_state.chat_messages.append({
                "role": "user", 
                "content": action_text,
                "action_type": "smart_action"
            })
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": result["full_response"],
                "action_result": result["action_result"],
                "model_info": result.get("model_info", {}),
                "timestamp": datetime.now().isoformat()
            })
            st.rerun()
        else:
            st.error(f"‚ùå {result['action_result']['message']}")

def execute_suggested_expert_query(query):
    """Execute a suggested expert query with enhanced processing."""
    # Add user message to chat
    st.session_state.chat_messages.append({
        "role": "user", 
        "content": query,
        "suggestion_type": "expert_suggested"
    })
    
    # Process with expert AI if available
    with st.spinner("üß† Expert AI analyzing your request..."):
        if hasattr(st.session_state.rag_agent, 'expert_query') and callable(getattr(st.session_state.rag_agent, 'expert_query')):
            try:
                result = st.session_state.rag_agent.expert_query(
                    query, 
                    auto_remediate=False  # Don't auto-execute for suggestions
                )
                
                # Add assistant response with expert analysis
                st.session_state.chat_messages.append({
                    "role": "assistant",
                    "content": result.get("expert_response", result.get("standard_response", "")),
                    "expert_analysis": result.get("expert_analysis", {}),
                    "system_health": result.get("system_health", {}),
                    "confidence": result.get("confidence", 0.0),
                    "timestamp": datetime.now().isoformat(),
                    "suggestion_type": "expert_response"
                })
                
            except Exception as e:
                # Fallback to standard processing
                result = st.session_state.rag_agent.query_with_actions(
                    query, 
                    st.session_state.remediation
                )
                
                st.session_state.chat_messages.append({
                    "role": "assistant",
                    "content": result["full_response"],
                    "action_result": result.get("action_result", {}),
                    "model_info": result.get("model_info", {}),
                    "timestamp": datetime.now().isoformat(),
                    "fallback": True
                })
        else:
            # Standard processing
            result = st.session_state.rag_agent.query_with_actions(
                query, 
                st.session_state.remediation
            )
            
            st.session_state.chat_messages.append({
                "role": "assistant",
                "content": result["full_response"],
                "action_result": result.get("action_result", {}),
                "model_info": result.get("model_info", {}),
                "timestamp": datetime.now().isoformat()
            })
    
    st.rerun()

def execute_suggested_query(query):
    """Execute a suggested query with enhanced processing."""
    # Add user message to chat
    st.session_state.chat_messages.append({
        "role": "user", 
        "content": query,
        "suggestion_type": "ai_suggested"
    })
    
    # Process with enhanced AI
    with st.spinner("üß† Processing AI suggestion..."):
        result = st.session_state.rag_agent.query_with_actions(
            query, 
            st.session_state.remediation
        )
        
        # Add assistant response
        st.session_state.chat_messages.append({
            "role": "assistant",
            "content": result["full_response"],
            "action_result": result.get("action_result", {}),
            "model_info": result.get("model_info", {}),
            "timestamp": datetime.now().isoformat()
        })
    
    st.rerun()

def export_conversation():
    """Export conversation with advanced analytics."""
    if not st.session_state.chat_messages:
        st.warning("No conversation to export")
        return
    
    # Create comprehensive export
    export_data = {
        "export_metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_messages": len(st.session_state.chat_messages),
            "model_used": getattr(st.session_state.rag_agent, 'model_name', 'unknown'),
            "conversation_id": f"k8s_ai_{int(time.time())}",
            "features": {
                "streaming": getattr(st.session_state.rag_agent, 'streaming_enabled', False),
                "function_calling": getattr(st.session_state.rag_agent, 'function_calling_enabled', False),
                "context_window": getattr(st.session_state.rag_agent, 'context_window', 0)
            }
        },
        "conversation": st.session_state.chat_messages,
        "analytics": {}
    }
    
    # Add analytics if available
    if hasattr(st.session_state.rag_agent, 'get_conversation_summary'):
        export_data["analytics"] = st.session_state.rag_agent.get_conversation_summary()
    
    # Convert to JSON
    export_json = json.dumps(export_data, indent=2, default=str)
    
    # Offer download
    st.download_button(
        label="üìÑ Download Advanced Chat Export",
        data=export_json,
        file_name=f"k8s_ai_conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )
    
    st.success("‚úÖ Conversation exported with full analytics")

def show_conversation_analytics():
    """Display advanced conversation analytics."""
    if not st.session_state.chat_messages:
        st.warning("No conversation data available")
        return
    
    st.subheader("üìä Advanced Conversation Analytics")
    
    # Calculate analytics
    user_messages = [msg for msg in st.session_state.chat_messages if msg["role"] == "user"]
    assistant_messages = [msg for msg in st.session_state.chat_messages if msg["role"] == "assistant"]
    
    action_messages = [msg for msg in assistant_messages if msg.get("action_result", {}).get("executed")]
    
    # Display metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Exchanges", len(user_messages))
        st.metric("Actions Executed", len(action_messages))
    
    with col2:
        avg_response_length = sum(len(msg["content"]) for msg in assistant_messages) / len(assistant_messages) if assistant_messages else 0
        st.metric("Avg Response Length", f"{avg_response_length:.0f} chars")
        
        if hasattr(st.session_state.rag_agent, 'response_time'):
            st.metric("Last Response Time", f"{st.session_state.rag_agent.response_time:.2f}s")
    
    with col3:
        if hasattr(st.session_state.rag_agent, 'tokens_generated'):
            st.metric("Tokens Generated", f"{st.session_state.rag_agent.tokens_generated:,}")
        
        action_success_rate = len(action_messages) / len(user_messages) * 100 if user_messages else 0
        st.metric("Action Success Rate", f"{action_success_rate:.1f}%")
    
    with col4:
        model_name = getattr(st.session_state.rag_agent, 'model_name', 'Unknown')
        st.metric("Current Model", model_name)
        
        context_usage = len(str(st.session_state.chat_messages))
        st.metric("Memory Usage", f"{context_usage:,} chars")
    
    # Conversation flow visualization
    if assistant_messages:
        st.subheader("üí¨ Conversation Flow")
        
        # Recent topics
        recent_topics = [msg["content"][:100] + "..." for msg in user_messages[-5:]]
        for i, topic in enumerate(recent_topics, 1):
            st.write(f"{i}. {topic}")
        
        # Action analysis
        if action_messages:
            st.subheader("‚ö° Action Analysis")
            action_types = {}
            for msg in action_messages:
                action_type = msg.get("action_result", {}).get("action_type", "unknown")
                action_types[action_type] = action_types.get(action_type, 0) + 1
            
            for action_type, count in action_types.items():
                st.write(f"‚Ä¢ {action_type}: {count} times")
    
    # Model performance over time
    if hasattr(st.session_state.rag_agent, 'get_conversation_summary'):
        summary = st.session_state.rag_agent.get_conversation_summary()
        st.subheader("üéØ Model Performance")
        st.json(summary)

def execute_quick_action(action_text):
    """Execute a quick action with enhanced AI processing."""
    execute_smart_action(action_text)

def logs_and_issues():
    st.header("üìã Cluster Logs & Issues")
    
    if not st.session_state.monitor:
        st.error("‚ùå Kubernetes monitor not available")
        st.info("This tab requires connection to the Kubernetes API")
        return
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Recent Issues")
        try:
            issues = st.session_state.monitor.get_recent_issues()
            
            if issues:
                for issue in issues:
                    severity_color = {
                        "critical": "üî¥",
                        "warning": "üü°", 
                        "info": "üîµ"
                    }.get(issue.get("severity", "info"), "üîµ")
                    
                    with st.expander(f"{severity_color} {issue['title']} - {issue['timestamp']}"):
                        st.write(f"**Resource:** {issue['resource']}")
                        st.write(f"**Namespace:** {issue['namespace']}")
                        st.write(f"**Description:** {issue['description']}")
                        
                        col_a, col_b = st.columns(2)
                        with col_a:
                            if st.button(f"üîß Auto-remediate", key=f"auto_{issue['id']}"):
                                remediate_issue(issue['id'])
                        with col_b:
                            if st.button(f"üîç Investigate", key=f"investigate_{issue['id']}"):
                                investigate_issue(issue['id'])
            else:
                st.info("No issues detected! üéâ")
        except Exception as e:
            st.error(f"Failed to fetch issues: {e}")
            st.info("Using mock data for demonstration")
            st.json({
                "issues": [
                    {"title": "Mock Issue", "severity": "info", "description": "This is demo data"}
                ]
            })
    
    with col2:
        st.subheader("Live Metrics")
        try:
            if st.session_state.monitor:
                metrics = st.session_state.monitor.get_cluster_metrics()
                
                st.metric("CPU Usage", f"{metrics.get('cpu_usage', 0):.1f}%")
                st.metric("Memory Usage", f"{metrics.get('memory_usage', 0):.1f}%")
                st.metric("Pod Count", metrics.get('pod_count', 0))
                st.metric("Node Count", metrics.get('node_count', 0))
            else:
                # Fallback metrics
                st.metric("CPU Usage", "N/A")
                st.metric("Memory Usage", "N/A")
                st.metric("Pod Count", "N/A")
                st.metric("Node Count", "N/A")
        except Exception as e:
            st.warning(f"Metrics unavailable: {e}")
        
        # Real-time log stream
        st.subheader("Live Logs")
        log_container = st.empty()
        
        if st.checkbox("Enable live logs"):
            try:
                if st.session_state.monitor:
                    logs = st.session_state.monitor.get_live_logs(limit=10)
                    log_text = "\n".join([f"[{log['timestamp']}] {log['message']}" for log in logs])
                    log_container.text_area("", value=log_text, height=200, disabled=True)
                else:
                    log_container.text_area("", value="Monitor not available", height=200, disabled=True)
            except Exception as e:
                log_container.text_area("", value=f"Error fetching logs: {e}", height=200, disabled=True)

def forecasting_dashboard():
    st.header("üìà Resource Forecasting & Node Optimization")
    
    if isinstance(st.session_state.forecaster, MockForecaster):
        st.warning("‚ö†Ô∏è Forecasting not available - install missing dependencies")
        st.info("This feature requires scikit-learn and other ML dependencies.")
        return
    
    # Forecast controls
    col1, col2, col3 = st.columns(3)
    with col1:
        forecast_days = st.selectbox("Forecast Period", [1, 3, 7, 14, 30], index=2)
    with col2:
        resource_type = st.selectbox("Resource Type", ["CPU", "Memory", "Storage"])
    with col3:
        if st.button("üîÆ Generate Forecast"):
            generate_forecast(forecast_days, resource_type)
    
    # Forecast visualization
    try:
        forecast_data = st.session_state.forecaster.get_latest_forecast()
        if forecast_data:
            fig = px.line(
                forecast_data, 
                x='timestamp', 
                y='value', 
                color='type',
                title=f"{resource_type} Usage Forecast"
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No forecast data available. Generate a forecast to see results.")
    except Exception as e:
        st.error(f"Error displaying forecast: {e}")
    
    # Node recommendations
    st.subheader("üéØ Pod Placement Recommendations")
    try:
        recommendations = st.session_state.forecaster.get_placement_recommendations()
        
        if recommendations:
            for rec in recommendations:
                with st.expander(f"üì¶ {rec['pod_name']} ‚Üí {rec['recommended_node']}"):
                    st.write(f"**Current Node:** {rec['current_node']}")
                    st.write(f"**Recommended Node:** {rec['recommended_node']}")
                    st.write(f"**Reason:** {rec['reason']}")
                    st.write(f"**Expected Improvement:** {rec['improvement']}")
                    
                    if st.button(f"Apply Recommendation", key=f"apply_{rec['pod_name']}"):
                        apply_placement_recommendation(rec)
        else:
            st.info("No placement recommendations available.")
    except Exception as e:
        st.error(f"Error getting recommendations: {e}")

def glusterfs_dashboard():
    st.header("üóÑÔ∏è GlusterFS Health Monitor")
    
    if isinstance(st.session_state.glusterfs, MockGlusterFSAnalyzer):
        st.warning("‚ö†Ô∏è GlusterFS monitoring not available")
        st.info("This feature requires GlusterFS to be installed and configured.")
        return
    
    # Health overview
    try:
        health_status = st.session_state.glusterfs.get_health_status()
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Volume Status", health_status.get('volumes_healthy', 'Unknown'))
        with col2:
            st.metric("Peer Status", health_status.get('peers_connected', 'Unknown'))
        with col3:
            st.metric("Heal Pending", health_status.get('heal_pending', 0))
        with col4:
            st.metric("Split-brain Files", health_status.get('split_brain_files', 0))
    except Exception as e:
        st.error(f"Error getting health status: {e}")
        return
    
    # Heal map visualization
    st.subheader("üó∫Ô∏è Heal Map")
    try:
        heal_data = st.session_state.glusterfs.get_heal_map()
        
        if heal_data:
            fig = go.Figure()
            
            for volume in heal_data:
                fig.add_trace(go.Scatter(
                    x=volume['timestamps'],
                    y=volume['heal_counts'],
                    mode='lines+markers',
                    name=volume['volume_name'],
                    line=dict(width=2)
                ))
            
            fig.update_layout(
                title="Heal Activity Over Time",
                xaxis_title="Time",
                yaxis_title="Files Healing",
                hovermode='x unified'
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No heal data available.")
    except Exception as e:
        st.error(f"Error displaying heal map: {e}")
    
    # Stuck peer detection
    st.subheader("üîç Peer Analysis")
    try:
        peers = st.session_state.glusterfs.analyze_peers()
        
        if peers:
            for peer in peers:
                status_icon = "üü¢" if peer['status'] == 'connected' else "üî¥"
                with st.expander(f"{status_icon} Peer: {peer['hostname']}"):
                    st.write(f"**Status:** {peer['status']}")
                    st.write(f"**UUID:** {peer['uuid']}")
                    st.write(f"**Last Seen:** {peer['last_seen']}")
                    
                    if peer['status'] != 'connected':
                        if st.button(f"üîß Attempt Reconnection", key=f"reconnect_{peer['uuid']}"):
                            reconnect_peer(peer['uuid'])
        else:
            st.info("No peer information available.")
    except Exception as e:
        st.error(f"Error analyzing peers: {e}")

def manual_remediation():
    st.header("‚öôÔ∏è Manual Remediation Tools")
    
    # Quick actions
    st.subheader("üöÄ Quick Actions")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("**Pod Operations**")
        if st.button("üîÑ Restart Failed Pods"):
            restart_failed_pods()
        if st.button("üìä Scale Deployment"):
            show_scaling_dialog()
        if st.button("üóëÔ∏è Clean Completed Jobs"):
            clean_completed_jobs()
    
    with col2:
        st.write("**Node Operations**")
        if st.button("üîß Drain Node"):
            show_node_drain_dialog()
        if st.button("üè∑Ô∏è Label Nodes"):
            show_node_labeling_dialog()
        if st.button("üìà Uncordon Nodes"):
            uncordon_all_nodes()
    
    with col3:
        st.write("**Storage Operations**")
        if st.button("üóÑÔ∏è Clean PV/PVC"):
            clean_orphaned_storage()
        if st.button("üîç Analyze Storage"):
            analyze_storage_usage()
        if st.button("üè• Volume Health Check"):
            check_volume_health()
    
    # Custom remediation
    st.subheader("üõ†Ô∏è Custom Remediation")
    
    with st.expander("Custom kubectl command"):
        kubectl_cmd = st.text_input("Enter kubectl command:")
        if st.button("Execute") and kubectl_cmd:
            result = st.session_state.remediation.execute_kubectl(kubectl_cmd)
            st.code(result)
    
    with st.expander("Remediation History"):
        history = st.session_state.remediation.get_history()
        for action in history:
            st.write(f"**{action['timestamp']}** - {action['action']} - Status: {action['status']}")

# Helper functions
def scan_for_issues():
    if not st.session_state.monitor:
        st.error("‚ùå Monitor not available - cannot scan for issues")
        return
        
    with st.spinner("Scanning cluster for issues..."):
        try:
            st.session_state.monitor.scan_for_issues()
            st.success("Scan completed!")
        except Exception as e:
            st.error(f"Scan failed: {e}")
    st.rerun()

def generate_report():
    if not st.session_state.monitor:
        st.error("‚ùå Monitor not available - cannot generate report")
        return
        
    with st.spinner("Generating comprehensive report..."):
        try:
            report = st.session_state.monitor.generate_report()
            st.download_button(
                label="üìÑ Download Report",
                data=report,
                file_name=f"k8s_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
        except Exception as e:
            st.error(f"Report generation failed: {e}")

def run_health_check():
    if not st.session_state.monitor:
        st.error("‚ùå Monitor not available - cannot run health check")
        return
        
    with st.spinner("Running health check..."):
        try:
            health = st.session_state.monitor.run_health_check()
            if health['overall_status'] == 'healthy':
                st.success("‚úÖ Cluster is healthy!")
            else:
                st.warning(f"‚ö†Ô∏è Issues detected: {health['issues_count']}")
        except Exception as e:
            st.error(f"Health check failed: {e}")

def remediate_issue(issue_id):
    if isinstance(st.session_state.remediation, MockRemediationEngine):
        st.warning("‚ùå Remediation not available in mock mode")
        return
        
    with st.spinner("Applying remediation..."):
        try:
            result = st.session_state.remediation.auto_remediate(issue_id)
            if result['success']:
                st.success(f"‚úÖ {result['message']}")
            else:
                st.error(f"‚ùå {result['message']}")
        except Exception as e:
            st.error(f"Remediation failed: {e}")

def investigate_issue(issue_id):
    if isinstance(st.session_state.rag_agent, MockRAGAgent):
        st.info("üîç Investigation: Using basic troubleshooting guidance (mock mode)")
        return
        
    with st.spinner("Investigating issue..."):
        try:
            investigation = st.session_state.rag_agent.investigate_issue(issue_id)
            st.info(f"üîç Investigation: {investigation}")
        except Exception as e:
            st.error(f"Investigation failed: {e}")

def generate_forecast(days, resource_type):
    if isinstance(st.session_state.forecaster, MockForecaster):
        st.warning("‚ùå Forecasting not available in mock mode")
        return
        
    with st.spinner(f"Generating {days}-day {resource_type} forecast..."):
        try:
            st.session_state.forecaster.generate_forecast(days, resource_type)
            st.success("Forecast generated!")
        except Exception as e:
            st.error(f"Forecast generation failed: {e}")
    st.rerun()

def apply_placement_recommendation(rec):
    with st.spinner("Applying placement recommendation..."):
        result = st.session_state.remediation.move_pod(
            rec['pod_name'], 
            rec['recommended_node']
        )
        if result['success']:
            st.success("‚úÖ Pod moved successfully!")
        else:
            st.error(f"‚ùå Failed to move pod: {result['message']}")

def reconnect_peer(uuid):
    with st.spinner("Attempting to reconnect peer..."):
        result = st.session_state.glusterfs.reconnect_peer(uuid)
        if result['success']:
            st.success("‚úÖ Peer reconnected!")
        else:
            st.error(f"‚ùå Failed to reconnect: {result['message']}")

def restart_failed_pods():
    with st.spinner("Restarting failed pods..."):
        result = st.session_state.remediation.restart_failed_pods()
        st.success(f"‚úÖ Restarted {result['count']} pods")

def clean_completed_jobs():
    with st.spinner("Cleaning completed jobs..."):
        result = st.session_state.remediation.clean_completed_jobs()
        st.success(f"‚úÖ Cleaned {result['count']} jobs")

def clean_orphaned_storage():
    with st.spinner("Cleaning orphaned storage..."):
        result = st.session_state.remediation.clean_orphaned_storage()
        st.success(f"‚úÖ Cleaned {result['count']} orphaned resources")

def show_scaling_dialog():
    with st.expander("Scale Deployment", expanded=True):
        deployments = st.session_state.monitor.get_deployments()
        selected_deployment = st.selectbox("Select Deployment", deployments)
        new_replicas = st.number_input("New Replica Count", min_value=0, max_value=100, value=3)
        
        if st.button("Scale"):
            result = st.session_state.remediation.scale_deployment(selected_deployment, new_replicas)
            if result['success']:
                st.success(f"‚úÖ Scaled {selected_deployment} to {new_replicas} replicas")
            else:
                st.error(f"‚ùå Failed to scale: {result['message']}")

def show_node_drain_dialog():
    with st.expander("Drain Node", expanded=True):
        nodes = st.session_state.monitor.get_nodes()
        selected_node = st.selectbox("Select Node to Drain", nodes)
        ignore_daemonsets = st.checkbox("Ignore DaemonSets", value=True)
        delete_local_data = st.checkbox("Delete Local Data", value=False)
        
        if st.button("Drain Node"):
            result = st.session_state.remediation.drain_node(
                selected_node, ignore_daemonsets, delete_local_data
            )
            if result['success']:
                st.success(f"‚úÖ Node {selected_node} drained successfully")
            else:
                st.error(f"‚ùå Failed to drain node: {result['message']}")

def show_node_labeling_dialog():
    with st.expander("Label Nodes", expanded=True):
        nodes = st.session_state.monitor.get_nodes()
        selected_node = st.selectbox("Select Node", nodes)
        label_key = st.text_input("Label Key")
        label_value = st.text_input("Label Value")
        
        if st.button("Apply Label") and label_key:
            result = st.session_state.remediation.label_node(
                selected_node, label_key, label_value
            )
            if result['success']:
                st.success(f"‚úÖ Applied label {label_key}={label_value} to {selected_node}")
            else:
                st.error(f"‚ùå Failed to apply label: {result['message']}")

def uncordon_all_nodes():
    with st.spinner("Uncordoning all nodes..."):
        result = st.session_state.remediation.uncordon_all_nodes()
        st.success(f"‚úÖ Uncordoned {result['count']} nodes")

def analyze_storage_usage():
    with st.spinner("Analyzing storage usage..."):
        analysis = st.session_state.monitor.analyze_storage_usage()
        
        # Display storage analysis in a nice format
        st.subheader("üìä Storage Analysis Results")
        
        for volume in analysis['volumes']:
            with st.expander(f"Volume: {volume['name']}"):
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Used", f"{volume['used_gb']:.1f} GB")
                    st.metric("Available", f"{volume['available_gb']:.1f} GB")
                with col2:
                    st.metric("Usage %", f"{volume['usage_percent']:.1f}%")
                    st.metric("Type", volume['storage_class'])

def check_volume_health():
    with st.spinner("Checking volume health..."):
        health_check = st.session_state.monitor.check_volume_health()
        
        st.subheader("üè• Volume Health Check Results")
        
        for volume in health_check['volumes']:
            status_icon = "üü¢" if volume['healthy'] else "üî¥"
            with st.expander(f"{status_icon} {volume['name']}"):
                st.write(f"**Status:** {volume['status']}")
                st.write(f"**Phase:** {volume['phase']}")
                if volume['issues']:
                    st.warning("Issues found:")
                    for issue in volume['issues']:
                        st.write(f"- {issue}")

# Mode-specific interface functions
def debug_analysis_interface(config_manager):
    """Debug mode interface for root cause analysis only"""
    st.subheader("üîç Deep Diagnostic Tools")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**System Analysis**")
        if st.button("üß¨ Deep Pod Analysis", key="debug_pods"):
            with st.spinner("Analyzing pods..."):
                debug_pod_analysis()
        
        if st.button("üî¨ Network Diagnostics", key="debug_network"):
            with st.spinner("Running network diagnostics..."):
                debug_network_analysis()
        
        if st.button("üíæ Storage Deep Dive", key="debug_storage"):
            with st.spinner("Analyzing storage..."):
                debug_storage_analysis()
    
    with col2:
        st.write("**Historical Analysis**")
        if st.button("üìä Pattern Analysis", key="debug_patterns"):
            show_debug_patterns()
        
        if st.button("üîÆ Root Cause Prediction", key="debug_prediction"):
            show_root_cause_prediction()
        
        if st.button("üìà Trend Analysis", key="debug_trends"):
            show_trend_analysis()
    
    # Debug-specific configurations
    st.subheader("üéõÔ∏è Debug Configuration")
    with st.expander("Debug Settings"):
        debug_level = st.selectbox("Debug Level", ["Basic", "Detailed", "Verbose", "Expert"])
        include_historical = st.checkbox("Include Historical Data", value=True)
        export_findings = st.checkbox("Export Findings", value=False)
        
        if st.button("Apply Debug Settings"):
            st.success(f"Debug mode configured: {debug_level} level")

def auto_remediation_interface(config_manager):
    """Remediation mode interface for automatic fixes"""
    st.subheader("üîß Automatic Remediation Dashboard")
    
    config = config_manager.get_current_config()
    
    # Remediation status
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Auto-Remediation", "‚úÖ Enabled" if config.auto_remediation else "‚ùå Disabled")
    with col2:
        confidence_threshold = config.remediation_confidence_threshold
        st.metric("Confidence Threshold", f"{confidence_threshold}%")
    with col3:
        st.metric("Safety Level", config.automation_level.value.replace('_', ' ').title())
    
    # Active remediations
    st.subheader("üöÄ Active Remediations")
    
    # Mock active remediation data
    if st.button("üîÑ Refresh Active Remediations"):
        active_remediations = get_active_remediations()
        
        if active_remediations:
            for remediation in active_remediations:
                with st.container():
                    st.write(f"**{remediation['type']}**: {remediation['description']}")
                    progress = st.progress(remediation['progress'])
                    st.caption(f"Status: {remediation['status']} | Confidence: {remediation['confidence']}%")
        else:
            st.info("No active remediations running")
    
    # Remediation history
    st.subheader("üìú Recent Remediation History")
    remediation_history = get_remediation_history()
    
    for item in remediation_history[:5]:  # Show last 5
        status_emoji = "‚úÖ" if item['success'] else "‚ùå"
        st.write(f"{status_emoji} **{item['type']}** - {item['timestamp']} - {item['description']}")
    
    # Quick remediation actions
    st.subheader("‚ö° Quick Fixes")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üîÑ Restart Failed Pods"):
            auto_restart_failed_pods()
    
    with col2:
        if st.button("üßπ Clean Resources"):
            auto_clean_resources()
    
    with col3:
        if st.button("‚öñÔ∏è Rebalance Workloads"):
            auto_rebalance_workloads()

def monitoring_interface(config_manager):
    """Monitoring mode interface for real-time monitoring"""
    st.subheader("üìä Real-time Cluster Monitoring")
    
    # Auto-refresh controls
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        refresh_interval = st.selectbox("Refresh Interval", [10, 30, 60, 120], index=1)
        st.caption(f"Next refresh in {refresh_interval} seconds")
    
    with col2:
        auto_refresh = st.toggle("Auto Refresh", value=True)
    
    with col3:
        if st.button("üîÑ Refresh Now"):
            st.rerun()
    
    # Real-time metrics
    st.subheader("üìà Live Metrics")
    
    # Create real-time metric displays
    metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
    
    with metric_col1:
        pod_count = get_live_pod_count()
        st.metric("Active Pods", pod_count, delta=get_pod_delta())
    
    with metric_col2:
        node_count = get_live_node_count()
        st.metric("Healthy Nodes", node_count, delta=get_node_delta())
    
    with metric_col3:
        alert_count = get_active_alert_count()
        st.metric("Active Alerts", alert_count, delta=get_alert_delta())
    
    with metric_col4:
        cpu_usage = get_cluster_cpu_usage()
        st.metric("Cluster CPU", f"{cpu_usage}%", delta=get_cpu_delta())
    
    # Real-time alerts
    st.subheader("üö® Live Alert Stream")
    alert_container = st.container()
    
    with alert_container:
        alerts = get_live_alerts()
        for alert in alerts[:10]:  # Show last 10 alerts
            severity_color = {"critical": "üî¥", "warning": "üü°", "info": "üîµ"}
            st.write(f"{severity_color.get(alert['severity'], '‚ö™')} **{alert['title']}** - {alert['timestamp']}")
            st.caption(alert['description'])
    
    # Auto-refresh logic
    if auto_refresh:
        import time
        time.sleep(refresh_interval)
        st.rerun()

def alert_dashboard():
    """Alert dashboard for monitoring mode"""
    st.subheader("üö® Alert Management Dashboard")
    
    # Alert summary
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        critical_alerts = get_critical_alert_count()
        st.metric("Critical", critical_alerts, delta=get_critical_delta())
    
    with col2:
        warning_alerts = get_warning_alert_count()
        st.metric("Warnings", warning_alerts, delta=get_warning_delta())
    
    with col3:
        info_alerts = get_info_alert_count()
        st.metric("Info", info_alerts, delta=get_info_delta())
    
    with col4:
        resolved_alerts = get_resolved_alert_count()
        st.metric("Resolved", resolved_alerts, delta=get_resolved_delta())
    
    # Alert management
    st.subheader("Alert Actions")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üîï Acknowledge All"):
            acknowledge_all_alerts()
    
    with col2:
        if st.button("‚úÖ Mark Resolved"):
            mark_alerts_resolved()
    
    with col3:
        if st.button("üìß Send Alert Summary"):
            send_alert_summary()

def learning_insights_interface():
    """Learning insights interface for debug mode"""
    st.subheader("üß† AI Learning Insights")
    
    # Learning status
    col1, col2, col3 = st.columns(3)
    
    with col1:
        pattern_count = get_learned_pattern_count()
        st.metric("Learned Patterns", pattern_count)
    
    with col2:
        confidence_avg = get_average_confidence()
        st.metric("Avg Confidence", f"{confidence_avg}%")
    
    with col3:
        prediction_accuracy = get_prediction_accuracy()
        st.metric("Prediction Accuracy", f"{prediction_accuracy}%")
    
    # Recent learning
    st.subheader("üìö Recent Learning Activity")
    learning_activity = get_recent_learning_activity()
    
    for activity in learning_activity:
        st.write(f"**{activity['pattern']}** - Learned from {activity['source']} at {activity['timestamp']}")
        st.caption(f"Confidence: {activity['confidence']}% | Instances: {activity['instances']}")

def system_analysis_interface():
    """System analysis interface for monitoring mode"""
    st.subheader("üîç System Analysis")
    
    # Analysis controls
    col1, col2 = st.columns(2)
    
    with col1:
        analysis_type = st.selectbox("Analysis Type", 
                                   ["Full System Scan", "Performance Analysis", "Security Audit", "Resource Usage"])
    
    with col2:
        if st.button("üöÄ Run Analysis"):
            run_system_analysis_by_type(analysis_type)
    
    # Analysis results
    st.subheader("üìä Analysis Results")
    show_analysis_results()

# Helper functions for mode-specific interfaces
def get_active_remediations():
    """Mock function to get active remediations"""
    return [
        {"type": "Pod Restart", "description": "Restarting failed nginx pod", "progress": 0.7, "status": "Running", "confidence": 95},
        {"type": "Node Drain", "description": "Draining node-01 for maintenance", "progress": 0.3, "status": "In Progress", "confidence": 88}
    ]

def get_remediation_history():
    """Mock function to get remediation history"""
    import datetime
    now = datetime.datetime.now()
    return [
        {"type": "Pod Restart", "description": "nginx-pod restarted successfully", "success": True, "timestamp": (now - datetime.timedelta(minutes=5)).strftime("%H:%M:%S")},
        {"type": "Resource Cleanup", "description": "Cleaned 3 orphaned PVCs", "success": True, "timestamp": (now - datetime.timedelta(minutes=15)).strftime("%H:%M:%S")},
        {"type": "Load Balancing", "description": "Failed to rebalance - insufficient resources", "success": False, "timestamp": (now - datetime.timedelta(minutes=30)).strftime("%H:%M:%S")}
    ]

def get_live_pod_count():
    """Mock function for live metrics"""
    import random
    return random.randint(45, 55)

def get_pod_delta():
    """Mock function for metric deltas"""
    import random
    return random.randint(-2, 3)

def get_live_node_count():
    import random
    return random.randint(8, 12)

def get_node_delta():
    import random
    return random.randint(-1, 1)

def get_active_alert_count():
    import random
    return random.randint(0, 5)

def get_alert_delta():
    import random
    return random.randint(-2, 2)

def get_cluster_cpu_usage():
    import random
    return random.randint(60, 85)

def get_cpu_delta():
    import random
    return random.randint(-5, 5)

def get_live_alerts():
    """Mock function for live alerts"""
    import datetime
    now = datetime.datetime.now()
    return [
        {"severity": "warning", "title": "High CPU Usage", "description": "Node-02 CPU usage at 85%", "timestamp": (now - datetime.timedelta(minutes=2)).strftime("%H:%M:%S")},
        {"severity": "info", "title": "Pod Scaled", "description": "nginx deployment scaled to 5 replicas", "timestamp": (now - datetime.timedelta(minutes=5)).strftime("%H:%M:%S")},
        {"severity": "critical", "title": "Node Unreachable", "description": "Node-03 is not responding", "timestamp": (now - datetime.timedelta(minutes=8)).strftime("%H:%M:%S")}
    ]

# Additional helper functions for the mode interfaces
def debug_pod_analysis():
    st.success("üß¨ Deep pod analysis completed - No automatic fixes applied (Debug Mode)")

def debug_network_analysis():
    st.success("üî¨ Network diagnostics completed - Analysis only (Debug Mode)")

def debug_storage_analysis():
    st.success("üíæ Storage deep dive completed - Findings documented (Debug Mode)")

def show_debug_patterns():
    st.info("üìä Displaying pattern analysis for debugging purposes")

def show_root_cause_prediction():
    st.info("üîÆ Showing predicted root causes based on historical data")

def show_trend_analysis():
    st.info("üìà Displaying trend analysis for system behavior")

def auto_restart_failed_pods():
    st.success("üîÑ Automatically restarted 3 failed pods")

def auto_clean_resources():
    st.success("üßπ Cleaned 5 orphaned resources automatically")

def auto_rebalance_workloads():
    st.success("‚öñÔ∏è Workloads rebalanced across cluster")

def get_critical_alert_count():
    import random
    return random.randint(0, 3)

def get_warning_alert_count():
    import random
    return random.randint(2, 8)

def get_info_alert_count():
    import random
    return random.randint(5, 15)

def get_resolved_alert_count():
    import random
    return random.randint(10, 25)

def get_critical_delta():
    import random
    return random.randint(-1, 1)

def get_warning_delta():
    import random
    return random.randint(-2, 3)

def get_info_delta():
    import random
    return random.randint(-5, 5)

def get_resolved_delta():
    import random
    return random.randint(0, 5)

def acknowledge_all_alerts():
    st.success("üîï All alerts acknowledged")

def mark_alerts_resolved():
    st.success("‚úÖ Selected alerts marked as resolved")

def send_alert_summary():
    st.success("üìß Alert summary sent to administrators")

def get_learned_pattern_count():
    import random
    return random.randint(15, 25)

def get_average_confidence():
    import random
    return random.randint(85, 95)

def get_prediction_accuracy():
    import random
    return random.randint(88, 96)

def get_recent_learning_activity():
    import datetime
    now = datetime.datetime.now()
    return [
        {"pattern": "Pod OOMKilled", "source": "nginx-logs", "timestamp": (now - datetime.timedelta(minutes=10)).strftime("%H:%M:%S"), "confidence": 92, "instances": 3},
        {"pattern": "Network Timeout", "source": "system-events", "timestamp": (now - datetime.timedelta(minutes=25)).strftime("%H:%M:%S"), "confidence": 87, "instances": 5}
    ]

def run_system_analysis_by_type(analysis_type):
    st.success(f"üöÄ {analysis_type} completed successfully")

def show_analysis_results():
    st.info("üìä System analysis results displayed based on monitoring data")

def get_mode_aware_response(prompt, rag_agent, remediation_engine, config_manager):
    """
    Generate mode-aware response that adapts based on current operational mode.
    UI always remains interactive regardless of backend configuration.
    
    Args:
        prompt: User input prompt
        rag_agent: RAG agent instance
        remediation_engine: Remediation engine instance
        config_manager: Configuration manager instance
        
    Returns:
        dict: Response with mode-aware behavior
    """
    if not config_manager:
        # Fallback to standard behavior if no config manager
        return rag_agent.query_with_actions(prompt, remediation_engine)
    
    config = config_manager.get_config()
    mode = config.mode.value
    
    # Get base response from RAG agent
    result = rag_agent.query_with_actions(prompt, remediation_engine)
    
    # Enhance response based on operational mode
    if mode == "debug":
        # Debug mode: Focus on analysis, mention no auto-remediation
        if "remediation" in result.get("full_response", "").lower():
            result["full_response"] += "\n\nüîç **Debug Mode**: Analysis complete. No automatic remediation will be performed. Use this information to understand the root cause."
        
        # Disable any auto-actions in debug mode
        if "actions" in result:
            result["actions"] = []
            result["debug_note"] = "Automatic actions disabled in debug mode - analysis only"
    
    elif mode == "remediation":
        # Remediation mode: Backend can auto-remediate, but UI still prompts
        if config.backend_auto_remediation:
            if "actions" in result and result["actions"]:
                result["full_response"] += f"\n\nüîß **Remediation Mode**: Backend auto-remediation is enabled (confidence threshold: {config.confidence_threshold}%). UI will prompt for confirmation before executing."
        else:
            result["full_response"] += "\n\nüîß **Remediation Mode**: Manual remediation mode - all actions require confirmation."
    
    elif mode == "interactive":
        # Interactive mode: Emphasize user control
        if "actions" in result and result["actions"]:
            result["full_response"] += "\n\nüí¨ **Interactive Mode**: All remediation actions require your explicit confirmation. Review the suggested actions below and approve those you want to execute."
    
    elif mode == "monitoring":
        # Monitoring mode: Focus on observation and alerts
        result["full_response"] += "\n\nüìä **Monitoring Mode**: Continuous monitoring active. Any issues detected will be tracked and alerted."
        
        # Add monitoring context
        if "actions" in result:
            monitoring_actions = []
            for action in result.get("actions", []):
                if action.get("type") in ["monitor", "alert", "log", "analyze"]:
                    monitoring_actions.append(action)
            result["actions"] = monitoring_actions
    
    elif mode == "hybrid":
        # Hybrid mode: Adaptive behavior
        if config.backend_auto_remediation:
            result["full_response"] += f"\n\nüîÑ **Hybrid Mode**: Adaptive operations enabled. High-confidence actions (>{config.confidence_threshold}%) can be auto-executed after UI confirmation."
        else:
            result["full_response"] += "\n\nüîÑ **Hybrid Mode**: Adaptive operations in manual mode - all actions require confirmation."
    
    # Always add UI interaction note
    result["ui_mode"] = "interactive"
    result["requires_user_confirmation"] = True
    result["mode_description"] = config_manager.get_mode_description()
    
    return result

def get_mode_aware_response(prompt, rag_agent, remediation, config_manager):
    """Generate mode-aware responses based on operational mode"""
    if not config_manager:
        # Fallback to standard response if no config manager
        return rag_agent.query_with_actions(prompt, remediation)
    
    mode = config_manager.current_mode.value
    config = config_manager.get_current_config()
    
    # Modify the prompt based on the operational mode
    if mode == 'debug':
        enhanced_prompt = f"""
        [DEBUG MODE - ANALYSIS ONLY]
        You are operating in debug mode. Focus on:
        - Root cause analysis
        - Detailed diagnostics
        - Historical pattern identification
        - NO automatic remediation actions
        
        User query: {prompt}
        
        Provide detailed analysis without suggesting any automatic fixes.
        """
    elif mode == 'remediation':
        enhanced_prompt = f"""
        [REMEDIATION MODE - AUTO-FIX ENABLED]
        You are operating in remediation mode with confidence threshold: {config.remediation_confidence_threshold}%
        
        You can:
        - Automatically fix issues above confidence threshold
        - Provide immediate remediation actions
        - Learn from successful remediations
        
        User query: {prompt}
        
        Provide solutions with confidence scores and auto-remediate if appropriate.
        """
    elif mode == 'monitoring':
        enhanced_prompt = f"""
        [MONITORING MODE - REAL-TIME FOCUS]
        You are operating in monitoring mode. Focus on:
        - Real-time system status
        - Alert generation and management
        - Continuous monitoring insights
        - Proactive issue detection
        
        User query: {prompt}
        
        Emphasize real-time monitoring and alerting capabilities.
        """
    elif mode == 'interactive':
        enhanced_prompt = f"""
        [INTERACTIVE MODE - USER CONFIRMATION]
        You are operating in interactive mode. Always:
        - Ask for user confirmation before actions
        - Provide clear explanations
        - Show step-by-step processes
        - Enable user control over all decisions
        
        User query: {prompt}
        
        Engage interactively and seek confirmation for any actions.
        """
    else:  # hybrid
        enhanced_prompt = f"""
        [HYBRID MODE - ADAPTIVE RESPONSE]
        You are operating in hybrid mode. Adapt your response based on:
        - Query complexity
        - Risk level of potential actions
        - User expertise level indicated
        
        User query: {prompt}
        
        Provide a balanced response with multiple options and approaches.
        """
    
    # Add mode context to the response
    try:
        result = rag_agent.query_with_actions(enhanced_prompt, remediation)
        
        # Add mode-specific context to the response
        mode_context = f"\n\nüéõÔ∏è **Current Mode**: {mode.title()} Mode\n"
        
        if mode == 'debug':
            mode_context += "üîç Analysis-only mode: No automatic actions will be taken.\n"
        elif mode == 'remediation':
            mode_context += f"üîß Auto-remediation enabled with {config.remediation_confidence_threshold}% confidence threshold.\n"
        elif mode == 'monitoring':
            mode_context += "üìä Real-time monitoring active with continuous alerts.\n"
        elif mode == 'interactive':
            mode_context += "üí¨ Interactive mode: All actions require your confirmation.\n"
        else:
            mode_context += "üîÑ Hybrid mode: Adaptive responses based on context.\n"
        
        # Enhance the response with mode context
        if 'full_response' in result:
            result['full_response'] = result['full_response'] + mode_context
        
        return result
        
    except Exception as e:
        # Fallback to standard response
        return rag_agent.query_with_actions(prompt, remediation)

if __name__ == "__main__":
    main()